{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model I: XGBoost\n",
    "\n",
    "XGBoost is trained using reg:squared. \n",
    "\n",
    "XGBoost has hyper parameter tuning by minimizing a metric that is closer to interpretale by humans. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import timedelta, datetime\n",
    "import itertools\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import yaml\n",
    "from tqdm import tqdm \n",
    "import mlflow\n",
    "from time import time\n",
    "import pickle\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK, plotting, space_eval\n",
    "\n",
    "# Get the current project path (where you open the notebook)\n",
    "# and go up two levels to get the project path\n",
    "current_dir = Path.cwd()\n",
    "proj_path = current_dir.parent.parent\n",
    "\n",
    "# make the code in src available to import in this notebook\n",
    "import sys\n",
    "sys.path.append(os.path.join(proj_path, 'src'))\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from metrics import *\n",
    "from utils import *\n",
    "\n",
    "\n",
    "# Catalog contains all the paths related to datasets\n",
    "with open(os.path.join(proj_path, 'conf/catalog.yml'), \"r\") as f:\n",
    "    catalog = yaml.safe_load(f)['olist']\n",
    "    \n",
    "# Params contains all of the dataset creation parameters and model parameters\n",
    "with open(os.path.join(proj_path, 'conf/params.yml'), \"r\") as f:\n",
    "    params = yaml.safe_load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "        # Range passed for learning rate\n",
    "       'eta': hp.quniform('eta', 0.02, 0.5, 0.01),\n",
    "        # Control overfitting\n",
    "        # Maximum depth of a tree: default 6 -> range: [0:âˆ]\n",
    "        'max_depth':  hp.choice('max_depth', np.arange(2, 10, dtype=int)),\n",
    "        # Minimum sum of instance weight (hessian) needed in a child: default 1\n",
    "        'min_child_weight': hp.quniform('min_child_weight', 1, 3, 1),\n",
    "        # Add randomness to make training robust to noise (control overfitting)\n",
    "        # Subsample ratio of the training instance: default 1\n",
    "        'subsample': hp.quniform('subsample', 0.2, 1, 0.1),\n",
    "        # Subsample ratio of columns when constructing each tree: default 1\n",
    "        'colsample_bytree': hp.quniform('colsample_bytree', 0.2, 1, 0.1),\n",
    "        'n_estimators': hp.choice('n_estimators', np.arange(5, 150, dtype=int))\n",
    "        }\n",
    "\n",
    "#Search function that will return the parameter values for the optimized score\n",
    "def optimize():\n",
    "       \n",
    "    # tpe.suggest is the algorithm used for updating the search space\n",
    "    # trials are used to log the information\n",
    "    # max_evals is used to specify how many combinations to look for\n",
    "\n",
    "    best = fmin(_score, space, algo=tpe.suggest, trials=trials, max_evals=params['xgb']['search_iter'], verbose=0)\n",
    "   \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing product category: bed_bath_table\n",
      "INFO: 'bed_bath_table' does not exist. Creating a new experiment\n",
      "  - - Processing range 2017-01-01 to 2018-01-28\n",
      "CPU times: user 47.2 s, sys: 1.96 s, total: 49.2 s\n",
      "Wall time: 7.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Step 1: Read data and convert string to proper datetime objects\n",
    "merged_data = pd.read_csv(os.path.join(proj_path, \n",
    "                                       catalog['output_dir']['dir'], \n",
    "                                       catalog['output_dir']['transactions']))\n",
    "\n",
    "merged_data['order_approved_at'] = pd.to_datetime(merged_data['order_approved_at'])\n",
    "\n",
    "# Step2: Create date folds\n",
    "date_ranges = make_dates(params['olist']['experiment_dates'])\n",
    "\n",
    "\n",
    "for prod_cat in params['olist']['product_categories']:\n",
    "    print(f'Processing product category: {prod_cat}')\n",
    "    \n",
    "    # Initialize mlflow tracking\n",
    "    create_folder(os.path.join(proj_path, 'mlruns'))\n",
    "    mlflow.set_tracking_uri(os.path.join(proj_path, 'mlruns'))\n",
    "    mlflow.set_experiment(prod_cat)\n",
    "    \n",
    "    start_timer = time()\n",
    "    all_predictions = []\n",
    "    all_hyperparameters = []\n",
    "    # Iterate over each period, unpack tuple in each variable.\n",
    "    # in each of the period, we will find the best set of parameters,\n",
    "    # which will represent the time-series cross validation methodology\n",
    "    for _, train_start, train_end, valid_start, valid_end, test_start, test_end in date_ranges.itertuples():\n",
    "        print(f'  - - Processing range {str(train_start.date())} to {str(test_end.date())}')\n",
    "\n",
    "        # allow the model to see past 6 months as features\n",
    "        lag_units = params['xgb']['window_size']\n",
    "        # different rolling averages as individual features\n",
    "        avg_units = params['xgb']['avg_units']\n",
    "\n",
    "        # Step x: First we filter out the stores and upcs, so that we obtain\n",
    "        # a dataset that only considers the same product. Then we can do some feature engineering\n",
    "        # which is required for xgboost. Because the initial features will result in NaNs,\n",
    "        # we do it first, and then we filter them for respective dates. \n",
    "        filtered_data = merged_data[(merged_data['product_category_name']==prod_cat)][['payment_value','order_approved_at']].copy()\n",
    "\n",
    "        # Add sale and date features (inplace)\n",
    "        # We do this on all the data, to avoid having NaN at the begining due to calculating the lags\n",
    "        make_lag_features(filtered_data, lag_units, col_name='payment_value', \n",
    "                          prefix_name='lag-payment_value', inplace=True)\n",
    "        \n",
    "        make_historical_avg(filtered_data, r_list=avg_units, col_n='lag-payment_value-1', google_trends=True)\n",
    "        add_datepart(filtered_data, fldname='order_approved_at', drop=False)\n",
    "\n",
    "        # Filter data\n",
    "        training_df = filtered_data[(filtered_data['order_approved_at']>=train_start) &\n",
    "                                    (filtered_data['order_approved_at']<=train_end)].copy()\n",
    "\n",
    "        valid_df = filtered_data[(filtered_data['order_approved_at']>=valid_start) &\n",
    "                                 (filtered_data['order_approved_at']<=valid_end)].copy()\n",
    "\n",
    "        test_df = filtered_data[(filtered_data['order_approved_at']>=test_start) &\n",
    "                                (filtered_data['order_approved_at']<=test_end)].copy()\n",
    "\n",
    "        training_df.set_index('order_approved_at', inplace=True)\n",
    "        valid_df.set_index('order_approved_at', inplace=True)\n",
    "        test_df.set_index('order_approved_at', inplace=True)\n",
    "\n",
    "        X_train = training_df\n",
    "        y_train = X_train.pop('payment_value')\n",
    "        X_valid = valid_df\n",
    "        y_valid = X_valid.pop('payment_value')\n",
    "        X_test = test_df\n",
    "        y_test = X_test.pop('payment_value')\n",
    "\n",
    "        #Function used to perform an evaluation on the validation set and return the score to the optimized function\n",
    "        def _score(params):\n",
    "            xg_boost_model = xgb.XGBRegressor(objective ='reg:squarederror',\n",
    "                                              colsample_bytree = params['colsample_bytree'],\n",
    "                                              learning_rate = params['eta'],\n",
    "                                              max_depth = params['max_depth'],\n",
    "                                              min_child_weight = params['min_child_weight'],\n",
    "                                              n_estimators = params['n_estimators'],\n",
    "                                              random_state = 2020,\n",
    "                                              subsample = params['subsample'],\n",
    "                                              tree_method = 'hist')\n",
    "            xg_boost_model.fit(X_train, y_train)\n",
    "            preds = xg_boost_model.predict(X_valid)\n",
    "            mape = mean_absolute_percentage_error(y_valid, preds)\n",
    "            \n",
    "            return mape\n",
    "        \n",
    "        trials = Trials()\n",
    "        best_hyperparams = optimize()\n",
    "        hyperparams = space_eval(space, best_hyperparams)\n",
    "\n",
    "        all_hyperparameters.extend(hyperparams)\n",
    "        xgb_model = XGBClassifier(hyperparams)\n",
    "        \n",
    "        # Concat training with validation data\n",
    "        xgb_model.fit(pd.concat([X_train, X_valid]), pd.concat([y_train, y_valid]))\n",
    "\n",
    "        test_preds = xgb_model.predict(X_test)\n",
    "        all_predictions.extend(test_preds)\n",
    "        \n",
    "        # Save plot importance for specific fold\n",
    "        rcParams.update({'figure.autolayout': True}) # for saving in correct size\n",
    "        fig, ax = plt.subplots(figsize=(16, 8))\n",
    "        xgb.plot_importance(xgb_model, max_num_features=15, ax=ax)\n",
    "        plot_dir = proj_path / catalog['extra']['dir'] / catalog['extra']['xgb_importance'] / f'{str(test_end.date())}'\n",
    "        plot_name = plot_dir / f'olist_xgb_exp1_{prod_cat}.png'\n",
    "        create_folder(plot_dir)      \n",
    "        plt.savefig(plot_name)\n",
    "\n",
    "    df_filtered = merged_data[(merged_data['product_category_name'] == prod_cat) &\n",
    "                              (merged_data['order_approved_at'] >= params['olist']['experiment_dates']['test_start']) & \n",
    "                              (merged_data['order_approved_at'] <= params['olist']['experiment_dates']['test_end'])].copy()\n",
    "    \n",
    "    test_metrics = get_metrics(df_filtered['payment_value'].values, all_predictions)\n",
    "    \n",
    "    save_data = pd.DataFrame({'y_true': df_filtered['payment_value'],\n",
    "                              'preds': all_predictions,\n",
    "                              'dates': df_filtered['order_approved_at']})\n",
    "    \n",
    "    fdir = os.path.join(proj_path, catalog['results']['dir'])\n",
    "    fname = os.path.join(fdir, f'xgb_exp1_{prod_cat}.csv')\n",
    "    create_folder(fdir)\n",
    "    save_data.to_csv(fname)\n",
    "            \n",
    "    fdir_hparam = os.path.join(proj_path, catalog['extra']['dir'], catalog['extra']['xgb_hyperparams'])\n",
    "    fname_hparam = os.path.join(fdir_hparam, f'xgb_exp1_{prod_cat}.csv')\n",
    "    create_folder(fdir_hparam)\n",
    "    \n",
    "    with open(fname_hparam, \"wb\") as filehandler:\n",
    "        pickle.dump(all_hyperparameters, filehandler)\n",
    "    duration_min = int((time() - start_timer) // 60)\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_artifact(fname)\n",
    "        mlflow.log_artifact(fname_hparam)\n",
    "        mlflow.log_param('model','xgb')\n",
    "        mlflow.log_metric('time', duration_min)\n",
    "        mlflow.log_param('add_date', 'true')\n",
    "        mlflow.log_params(params['xgb'])\n",
    "        mlflow.log_metrics(test_metrics)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
