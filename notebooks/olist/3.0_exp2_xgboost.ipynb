{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model I: XGBoost\n",
    "\n",
    "By default, it will combine all the google trends series related to the product category and the cross_category series.  \n",
    "\n",
    "Hyperparameter search is performed using Hyperopt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import timedelta, datetime\n",
    "import itertools\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import yaml\n",
    "from tqdm import tqdm \n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK, plotting, space_eval\n",
    "import mlflow\n",
    "from time import time\n",
    "import pickle\n",
    "\n",
    "# Get the current project path (where you open the notebook)\n",
    "# and go up two levels to get the project path\n",
    "current_dir = Path.cwd()\n",
    "proj_path = current_dir.parent.parent\n",
    "\n",
    "# make the code in src available to import in this notebook\n",
    "import sys\n",
    "sys.path.append(os.path.join(proj_path,'src'))\n",
    "from glob import glob\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from metrics import *\n",
    "from utils import *\n",
    "\n",
    "\n",
    "# Catalog contains all the paths related to datasets\n",
    "with open(os.path.join(proj_path, 'conf/catalog.yml'), \"r\") as f:\n",
    "    catalog = yaml.safe_load(f)['olist']\n",
    "    \n",
    "# Params contains all of the dataset creation parameters and model parameters\n",
    "with open(os.path.join(proj_path, 'conf/params.yml'), \"r\") as f:\n",
    "    params = yaml.load(f,Loader=yaml.Loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "        # Range passed for learning rate\n",
    "       'eta': hp.quniform('eta', 0.02, 0.5, 0.01),\n",
    "        # Control overfitting\n",
    "        # Maximum depth of a tree: default 6 -> range: [0:∞]\n",
    "        'max_depth':  hp.choice('max_depth', np.arange(2, 10, dtype=int)),\n",
    "        # Minimum sum of instance weight (hessian) needed in a child: default 1\n",
    "        'min_child_weight': hp.quniform('min_child_weight', 1, 3, 1),\n",
    "        # Add randomness to make training robust to noise (control overfitting)\n",
    "        # Subsample ratio of the training instance: default 1\n",
    "        'subsample': hp.quniform('subsample', 0.2, 1, 0.1),\n",
    "        # Subsample ratio of columns when constructing each tree: default 1\n",
    "        'colsample_bytree': hp.quniform('colsample_bytree', 0.2, 1, 0.1),\n",
    "        'n_estimators': hp.choice('n_estimators', np.arange(5, 150, dtype=int))\n",
    "        }\n",
    "\n",
    "#Search function that will return the parameter values for the optimized score\n",
    "def optimize():\n",
    "       \n",
    "    # tpe.suggest is the algorithm used for updating the search space\n",
    "    # trials are used to log the information\n",
    "    # max_evals is used to specify how many combinations to look for\n",
    "\n",
    "    best = fmin(_score, space, algo=tpe.suggest, trials=trials, max_evals=params['xgb']['search_iter'], verbose=0)\n",
    "   \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing product category: bed_bath_table\n",
      "  - - Processing range 2017-01-01 to 2018-01-28\n",
      "CPU times: user 1min 36s, sys: 4.17 s, total: 1min 41s\n",
      "Wall time: 15.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Step 1: Read data and convert string to proper datetime objects\n",
    "merged_data = pd.read_csv(os.path.join(proj_path, \n",
    "                                       catalog['output_dir']['dir'], \n",
    "                                       catalog['output_dir']['transactions']))\n",
    "\n",
    "merged_data['order_approved_at'] = pd.to_datetime(merged_data['order_approved_at'])\n",
    "\n",
    "# Step2: Create date folds\n",
    "date_ranges = make_dates(params['olist']['experiment_dates'])\n",
    "\n",
    "for prod_cat in params['olist']['product_categories']:\n",
    "    print(f'Processing product category: {prod_cat}')\n",
    "    \n",
    "    # Initialize mlflow tracking\n",
    "    create_folder(os.path.join(proj_path, 'mlruns'))\n",
    "    mlflow.set_tracking_uri(os.path.join(proj_path, 'mlruns'))\n",
    "    mlflow.set_experiment(prod_cat)\n",
    "    \n",
    "    start_timer = time()\n",
    "    all_predictions = []\n",
    "    all_hyperparameters = []\n",
    "    \n",
    "    # Iterate over each period, unpack tuple in each variable.\n",
    "    # in each of the period, we will find the best set of parameters,\n",
    "    # which will represent the time-series cross validation methodology\n",
    "    for _, train_start, train_end, valid_start, valid_end, test_start, test_end in date_ranges.itertuples():\n",
    "        print(f'  - - Processing range {str(train_start.date())} to {str(test_end.date())}')\n",
    "\n",
    "        \n",
    "        # allow the model to see past 6 months as features\n",
    "        lag_units = params['xgb']['window_size']\n",
    "        avg_units = params['xgb']['avg_units']\n",
    "\n",
    "        filtered_data = merged_data[(merged_data['product_category_name']==prod_cat)][['payment_value','order_approved_at']].copy()\n",
    "\n",
    "\n",
    "        # ┌────────────────── GOOGLE TRENDS STARTS HERE ────────────────────┐\n",
    "        # │\n",
    "        # Get product category and state\n",
    "        state = 'SP'\n",
    "#         cat = params['breakfast']['dataset']['upc_ids'][upc_id]\n",
    "\n",
    "        # List comprehension to iteratively read in the correct directory the google\n",
    "        # trends csv files that match the appropriate pattern of * (everything) in respective state\n",
    "        # returns a list of pandas dataframes where after we concat on the column axis using the index_col\n",
    "        # After that, we reset the index, to get the Week as a column and convert to datetime\n",
    "        # Finaly, we add 6 days as the google trends hits represents the end of week and corresponds\n",
    "        # to Breakfast At the Frat dates\n",
    "        \n",
    "        \n",
    "        # Category Specific Google Trends Series\n",
    "        glist = [\n",
    "            pd.read_csv(path, \n",
    "                        skiprows= 3, \n",
    "                        index_col= 'Week', \n",
    "                        names= ['Week', parse_name([str(proj_path), \n",
    "                                                    f\"/{str(catalog['google_trends']['dir'])}/{prod_cat}/\", \n",
    "                                                    '.csv'],path)]) \n",
    "            for path in glob(str(proj_path / catalog['google_trends']['dir'] / f'{prod_cat}/*_{state}.csv'))\n",
    "        ]\n",
    "        glist_br = [\n",
    "            pd.read_csv(path, \n",
    "                        skiprows= 3, \n",
    "                        index_col= 'Week', \n",
    "                        names= ['Week', parse_name([str(proj_path), \n",
    "                                                    f\"/{str(catalog['google_trends']['dir'])}/{prod_cat}/\", \n",
    "                                                    '.csv'],path)]) \n",
    "            for path in glob(str(proj_path / catalog['google_trends']['dir'] / f'{prod_cat}/*_BR.csv'))\n",
    "        ]\n",
    "        \n",
    "        # Cross Category\n",
    "        glist_cc_state = [\n",
    "            pd.read_csv(path, \n",
    "                        skiprows= 3, \n",
    "                        index_col= 'Week', \n",
    "                        names= ['Week', parse_name([str(proj_path), \n",
    "                                                    f\"/{str(catalog['google_trends']['dir'])}/cross_category/\", \n",
    "                                                    '.csv'],path)]) \n",
    "            for path in glob(str(proj_path / catalog['google_trends']['dir'] / f'cross_category/*_{state}.csv'))\n",
    "        ]\n",
    "        \n",
    "        glist_cc_br = [\n",
    "            pd.read_csv(path, \n",
    "                        skiprows= 3, \n",
    "                        index_col= 'Week', \n",
    "                        names= ['Week', parse_name([str(proj_path), \n",
    "                                                    f\"/{str(catalog['google_trends']['dir'])}/cross_category/\", \n",
    "                                                    '.csv'],path)]) \n",
    "            for path in glob(str(proj_path / catalog['google_trends']['dir'] / 'cross_category/*_BR.csv'))\n",
    "        ]        \n",
    "        \n",
    "\n",
    "        # This is where you decide on whether to add those series, comment if you do not\n",
    "        # The series should all have unique names, and is based on name of the file\n",
    "        glist.extend(glist_br)\n",
    "        glist.extend(glist_cc_state)\n",
    "        glist.extend(glist_cc_br)\n",
    "\n",
    "        # Combine all of the series into one, column-wise\n",
    "        gdata = pd.concat(glist, axis=1).reset_index()\n",
    "        gdata = fix_col_syntax(gdata)\n",
    "        # reset index will always set the Week as the first column, so select after that date.\n",
    "        gdata_feature_cols = gdata.columns[1:].to_list()\n",
    "        gdata['Week'] = pd.to_datetime(gdata['Week'])\n",
    "        # + 7 days to get end of week hits, then + 7 again to allign gtrends (t-1) to series (t)\n",
    "        gdata['Week'] = gdata['Week'] + timedelta(days=14) \n",
    "\n",
    "        for col in gdata:\n",
    "            if gdata[col].dtype == 'O':\n",
    "                gdata[col] = [0 if row_item == '<1' else int(row_item) for row_item in gdata[col]]\n",
    "                \n",
    "        # Join on filtered_data\n",
    "        filtered_data = filtered_data.merge(gdata, left_on='order_approved_at', \n",
    "                                            right_on='Week', how='left').drop(columns='Week')\n",
    "\n",
    "        # Create lagged features of google trends series\n",
    "        # There isn't any leakage this way as we are predicting next week. dont need to calclate on lag-1\n",
    "        for gt_feature_col in gdata_feature_cols:\n",
    "            make_lag_features(filtered_data, params['xgb']['gtrends_window_size'], col_name=gt_feature_col, \n",
    "                              prefix_name=gt_feature_col, inplace=True) # used to be f'lag-{gt_feature_col}-1'\n",
    "            \n",
    "            make_historical_avg(filtered_data, r_list=avg_units, \n",
    "                                col_n=gt_feature_col, google_trends=True) # used to be f'lag-{gt_feature_col}-1'\n",
    "#             _ = filtered_data.pop(gt_feature_col) # drop so no leakage\n",
    "        # │ \n",
    "        # └────────────────────── GOOGLE TRENDS ENDS HERE ─────────────────────────┘\n",
    "\n",
    "        # Add sale and date features (inplace) \n",
    "        # We do this on all the data, to avoid having NaN at the begining due to calculating the lags\n",
    "        make_lag_features(filtered_data,lag_units,col_name='payment_value', \n",
    "                          prefix_name='lag-payment_value',inplace=True)\n",
    "        make_historical_avg(filtered_data, r_list=avg_units, col_n='lag-payment_value-1', google_trends=True)\n",
    "        add_datepart(filtered_data, fldname='order_approved_at', drop=False)\n",
    "        # Filter data\n",
    "        training_df = filtered_data[(filtered_data['order_approved_at']>=train_start) &\n",
    "                                    (filtered_data['order_approved_at']<=train_end)].copy()\n",
    "\n",
    "        valid_df = filtered_data[(filtered_data['order_approved_at']>=valid_start) &\n",
    "                                 (filtered_data['order_approved_at']<=valid_end)].copy()\n",
    "\n",
    "        test_df = filtered_data[(filtered_data['order_approved_at']>=test_start) &\n",
    "                                (filtered_data['order_approved_at']<=test_end)].copy()\n",
    "        training_df.set_index('order_approved_at', inplace=True)\n",
    "        valid_df.set_index('order_approved_at', inplace=True)\n",
    "        test_df.set_index('order_approved_at', inplace=True)\n",
    "\n",
    "        X_train = training_df\n",
    "        y_train = X_train.pop('payment_value')\n",
    "        X_valid = valid_df\n",
    "        y_valid = X_valid.pop('payment_value')\n",
    "        X_test = test_df\n",
    "        y_test = X_test.pop('payment_value')\n",
    "\n",
    "        #Function used to perform an evaluation on the validation set and return \n",
    "        # the score to the optimized function\n",
    "        def _score(params):\n",
    "            xg_boost_model = xgb.XGBRegressor(objective ='reg:squarederror',\n",
    "                                              colsample_bytree = params['colsample_bytree'],\n",
    "                                              learning_rate = params['eta'],\n",
    "                                              max_depth = params['max_depth'],\n",
    "                                              min_child_weight = params['min_child_weight'],\n",
    "                                              n_estimators = params['n_estimators'],\n",
    "                                              random_state = 2020,\n",
    "                                              subsample = params['subsample'],\n",
    "                                              tree_method = 'hist')\n",
    "            xg_boost_model.fit(X_train, y_train)\n",
    "            preds = xg_boost_model.predict(X_valid)\n",
    "            mape = mean_absolute_percentage_error(y_valid, preds)\n",
    "            return mape\n",
    "        \n",
    "        trials = Trials()\n",
    "        best_hyperparams = optimize()\n",
    "        hyperparams = space_eval(space, best_hyperparams)\n",
    "        all_hyperparameters.extend(hyperparams)\n",
    "\n",
    "        xgb_model = XGBClassifier(hyperparams)\n",
    "        # Concat training with validation data\n",
    "        \n",
    "        xgb_model.fit(pd.concat([X_train, X_valid]), pd.concat([y_train, y_valid]))\n",
    "\n",
    "        test_preds = xgb_model.predict(X_test)\n",
    "        \n",
    "        all_predictions.extend(test_preds)\n",
    "        \n",
    "        # Save plot importance\n",
    "        rcParams.update({'figure.autolayout': True}) # for saving in correct size\n",
    "        fig, ax = plt.subplots(figsize=(16, 8))\n",
    "        xgb.plot_importance(xgb_model, max_num_features=15, ax=ax)\n",
    "        plot_dir = proj_path / catalog['extra']['dir'] / catalog['extra']['xgb_importance'] / f'{str(test_end.date())}'\n",
    "        plot_name = plot_dir / f'xgb_exp2_{prod_cat}.png'\n",
    "        create_folder(plot_dir)        \n",
    "        plt.savefig(plot_name)\n",
    "    \n",
    "    df_filtered = merged_data[(merged_data['product_category_name'] == prod_cat) &\n",
    "                              (merged_data['order_approved_at'] >= params['olist']['experiment_dates']['test_start']) & \n",
    "                              (merged_data['order_approved_at'] <= params['olist']['experiment_dates']['test_end'])].copy()\n",
    "    \n",
    "    test_metrics = get_metrics(df_filtered['payment_value'].values, all_predictions)\n",
    "    \n",
    "    save_data = pd.DataFrame({'y_true': df_filtered['payment_value'],\n",
    "                              'preds': all_predictions,\n",
    "                              'dates': df_filtered['order_approved_at']})\n",
    "    \n",
    "    fdir = os.path.join(proj_path, catalog['results']['dir'])\n",
    "    fname = os.path.join(fdir, f'xgb_exp2_{prod_cat}.csv')\n",
    "    create_folder(fdir)\n",
    "    save_data.to_csv(fname)\n",
    "\n",
    "    fdir_hparam = os.path.join(proj_path, catalog['extra']['dir'], catalog['extra']['xgb_hyperparams'])\n",
    "    fname_hparam = os.path.join(fdir_hparam, f'xgb_exp2_{prod_cat}.csv')\n",
    "    create_folder(fdir_hparam)\n",
    "    \n",
    "    with open(fname_hparam, \"wb\") as filehandler:\n",
    "        pickle.dump(all_hyperparameters, filehandler)\n",
    "    \n",
    "    duration_min = int((time() - start_timer) // 60)\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_artifact(fname)\n",
    "        mlflow.log_artifact(fname_hparam)\n",
    "        mlflow.log_param('model','xgb')\n",
    "        mlflow.log_param('add_date', 'true')\n",
    "        mlflow.log_params(params['xgb'])\n",
    "        mlflow.log_metrics(test_metrics)\n",
    "        mlflow.log_metric('time', duration_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
