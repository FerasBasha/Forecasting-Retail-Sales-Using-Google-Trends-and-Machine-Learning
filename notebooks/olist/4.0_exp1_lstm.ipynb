{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\feras\\Anaconda3\\envs\\ForecastingRetailSales\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\feras\\Anaconda3\\envs\\ForecastingRetailSales\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\feras\\Anaconda3\\envs\\ForecastingRetailSales\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\feras\\Anaconda3\\envs\\ForecastingRetailSales\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\feras\\Anaconda3\\envs\\ForecastingRetailSales\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\feras\\Anaconda3\\envs\\ForecastingRetailSales\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\feras\\Anaconda3\\envs\\ForecastingRetailSales\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\feras\\Anaconda3\\envs\\ForecastingRetailSales\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\feras\\Anaconda3\\envs\\ForecastingRetailSales\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\feras\\Anaconda3\\envs\\ForecastingRetailSales\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\feras\\Anaconda3\\envs\\ForecastingRetailSales\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\feras\\Anaconda3\\envs\\ForecastingRetailSales\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('CRITICAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import timedelta, datetime\n",
    "import itertools\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import yaml\n",
    "from time import time\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from scipy.stats.distributions import uniform, randint\n",
    "import mlflow\n",
    "from glob import glob\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "\n",
    "\n",
    "# Get the current project path (where you open the notebook)\n",
    "# and go up two levels to get the project path\n",
    "current_dir = Path.cwd()\n",
    "proj_path = current_dir.parent.parent\n",
    "\n",
    "\n",
    "# make the code in src available to import in this notebook\n",
    "import sys\n",
    "sys.path.append(os.path.join(proj_path,'src'))\n",
    "\n",
    "from metrics import *\n",
    "from utils import *\n",
    "from scalers import * \n",
    "\n",
    "# Catalog contains all the paths related to datasets\n",
    "with open(os.path.join(proj_path, 'conf/catalog.yml'), \"r\") as f:\n",
    "    catalog = yaml.safe_load(f)['olist']\n",
    "    \n",
    "# Params contains all of the dataset creation parameters and model parameters\n",
    "with open(os.path.join(proj_path, 'conf/params.yml'), \"r\") as f:\n",
    "    params = yaml.safe_load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\feras\\Anaconda3\\envs\\ForecastingRetailSales\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def set_model(n_layers:int=1, init_units:int=50, n_unit_strategy:str='stable', \n",
    "              dropout_p:float=0.1, num_timesteps: int=8, num_series: int=5, \n",
    "              lr:float=0.001, optimizer:str='adam', loss:str='mape'):\n",
    "    '''\n",
    "    # Gives flexibility\n",
    "    # Explore depth vs width of the RNN model\n",
    "    https://stackoverflow.com/questions/59072728/what-is-the-rule-to-know-how-many-lstm-cells-and-how-many-units-in-each-lstm-cel\n",
    "    # there are no rule of thumb, but here the decrease strategy will devide by i the number of units at each layer.\n",
    "    \n",
    "    Args\n",
    "        num_timesteps: the number of lags in the dataframes\n",
    "        num_series: the number of time series, specify 1 if univariate\n",
    "        n_unit_strategy: two options, stable and decrease. decrease will decrease the  \n",
    "    '''\n",
    "    model = Sequential()\n",
    "    if n_unit_strategy == 'stable':\n",
    "        n_units = [init_units] * n_layers\n",
    "    if n_unit_strategy == 'decrease':\n",
    "        n_units = [max(int(init_units/i),1) for i in range(1,n_layers+1)]\n",
    "    \n",
    "    if n_layers > 1:\n",
    "        model.add(LSTM(n_units[0], return_sequences=True, input_shape=(num_timesteps, num_series)))\n",
    "        model.add(Dropout(dropout_p)) # Prevent overfitting\n",
    "        for i in range(1, n_layers):\n",
    "            model.add(LSTM(n_units[i], return_sequences=True))\n",
    "            model.add(Dropout(dropout_p)) \n",
    "        model.add(LSTM(n_units[-1]))\n",
    "        model.add(Dropout(dropout_p))\n",
    "    \n",
    "    else:\n",
    "        model.add(LSTM(n_units[0], return_sequences=False, input_shape=(num_timesteps, num_series)))\n",
    "        model.add(Dropout(dropout_p)) # Prevent overfitting\n",
    "\n",
    "    model.add(Dense(units=1))\n",
    "    \n",
    "    if optimizer == 'adam':\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    if optimizer == 'rmsprop':\n",
    "        opt = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "    \n",
    "    model.compile(optimizer=opt, loss=loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\feras\\Anaconda3\\envs\\ForecastingRetailSales\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py:538: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  tensor_proto.tensor_content = nparray.tostring()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 8, 50)             11200     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 8, 50)             0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 8, 50)             20200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 50)             0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 8, 50)             20200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 50)             0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 71,851\n",
      "Trainable params: 71,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_a = set_model(n_layers=3, n_unit_strategy='stable')\n",
    "model_a.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, lag_units, list_features:list=['UNITS']):\n",
    "    \"\"\"\n",
    "    For every feature in the list of features, create lagged features\n",
    "    for a specified number of lagged units. The lenght must be the\n",
    "    same for all of the features. After creating the new features,\n",
    "    it filters on those and adds them to a list that will be \n",
    "    reshaped in the correct format.\n",
    "    \n",
    "    We go from a dataframe version:\n",
    "    \n",
    "    UNITS    | FEATURE_A | FEATURE_B |   ...\n",
    "    ---------|-----------|-----------|-------\n",
    "    \n",
    "    For every feature:\n",
    "        \n",
    "        Create the lagged features:\n",
    "\n",
    "        UNITS    | FEATURE_A | FEATURE_A-lag-1 |   ...\n",
    "        ---------|-----------|-----------------|-------  \n",
    "\n",
    "        Filter on each lagged feature and inverse columns:\n",
    "\n",
    "        FEATURE_A-lag-N | ... |  FEATURE_A-lag-1\n",
    "        ----------------|-----|------------------\n",
    "        \n",
    "        Convert to numpy as reshape by adding a dimension\n",
    "        (nrows, nlags) -> (nrows, nlags, 1)\n",
    "        \n",
    "    Concatenate each \"feature-specific matrix\" on the last dimension\n",
    "    \n",
    "    [(nrows, lagged-features-A, 1),\n",
    "     (nrows, lagged-features-B, 1),\n",
    "                  ...\n",
    "     (nrows, lagged-features-X, 1)]\n",
    "     \n",
    "     Results in: \n",
    "     \n",
    "     (nrows, nlags, N)\n",
    "    \n",
    "    \n",
    "    Note: We need to specify the column 'UNITS' in the list of features.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize a list\n",
    "    list_df = []\n",
    "    \n",
    "    for feature in list_features:\n",
    "        # create new features that will have the convention lag-<col_name>-<lagged unit>\n",
    "        make_lag_features(df, lag_units, col_name=feature, prefix_name=f'lag-{feature}',inplace=True)\n",
    "        # filter columns that were just created\n",
    "        # we are only appending lag something, so not actual features\n",
    "        dummy = df.filter(like=f'lag-{feature}')\n",
    "        # invert columns left to right (lag-1 ... lag-N) -> (lag-N ... lag-1)\n",
    "        dummy = dummy[dummy.columns[::-1]]\n",
    "        # add an extra dimension\n",
    "        dummy = dummy.values.reshape(dummy.shape[0], dummy.shape[1], 1)\n",
    "        # add the matrice to a list where it will get concatenated at the end\n",
    "        list_df.append(dummy)\n",
    "\n",
    "    # matrices in list_df must have shape (n_rows, n_cols, 1)\n",
    "    # so we can concatenate on the last axis\n",
    "    x_features = np.concatenate(list_df, axis=2)\n",
    "    y_feature = np.array(df['payment_value'].values)\n",
    "\n",
    "    return x_features, y_feature, df['order_approved_at'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing product category: bed_bath_table\n",
      "  - - Processing range 2017-01-01 to 2018-01-28\n",
      "  - - Processing range 2017-01-29 to 2018-02-25\n",
      "  - - Processing range 2017-02-26 to 2018-03-25\n",
      "  - - Processing range 2017-03-26 to 2018-04-22\n",
      "  - - Processing range 2017-04-23 to 2018-05-20\n",
      "  - - Processing range 2017-05-21 to 2018-06-17\n",
      "  - - Processing range 2017-06-18 to 2018-07-15\n",
      "  - - Processing range 2017-07-16 to 2018-08-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020/12/12 16:38:24 WARNING mlflow.tracking.context.git_context: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "The git executable must be specified in one of the following ways:\n",
      "    - be included in your $PATH\n",
      "    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "    - explicitly set via git.refresh()\n",
      "\n",
      "All git commands will error until this is rectified.\n",
      "\n",
      "This initial warning can be silenced or aggravated in the future by setting the\n",
      "$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "    - quiet|q|silence|s|none|n|0: for no warning or exception\n",
      "    - warn|w|warning|1: for a printed warning\n",
      "    - error|e|raise|r|2: for a raised exception\n",
      "\n",
      "Example:\n",
      "    export GIT_PYTHON_REFRESH=quiet\n",
      "\n",
      "C:\\Users\\feras\\Anaconda3\\envs\\ForecastingRetailSales\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py:538: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  tensor_proto.tensor_content = nparray.tostring()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing product category: health_beauty\n",
      "  - - Processing range 2017-01-01 to 2018-01-28\n",
      "  - - Processing range 2017-01-29 to 2018-02-25\n",
      "  - - Processing range 2017-02-26 to 2018-03-25\n",
      "  - - Processing range 2017-03-26 to 2018-04-22\n",
      "  - - Processing range 2017-04-23 to 2018-05-20\n",
      "  - - Processing range 2017-05-21 to 2018-06-17\n",
      "  - - Processing range 2017-06-18 to 2018-07-15\n",
      "  - - Processing range 2017-07-16 to 2018-08-12\n",
      "Processing product category: sports_leisure\n",
      "  - - Processing range 2017-01-01 to 2018-01-28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\feras\\Anaconda3\\envs\\ForecastingRetailSales\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py:538: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  tensor_proto.tensor_content = nparray.tostring()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - - Processing range 2017-01-29 to 2018-02-25\n",
      "  - - Processing range 2017-02-26 to 2018-03-25\n",
      "  - - Processing range 2017-03-26 to 2018-04-22\n",
      "  - - Processing range 2017-04-23 to 2018-05-20\n",
      "  - - Processing range 2017-05-21 to 2018-06-17\n",
      "  - - Processing range 2017-06-18 to 2018-07-15\n",
      "  - - Processing range 2017-07-16 to 2018-08-12\n",
      "Processing product category: furniture_decor\n",
      "  - - Processing range 2017-01-01 to 2018-01-28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\feras\\Anaconda3\\envs\\ForecastingRetailSales\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py:538: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  tensor_proto.tensor_content = nparray.tostring()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - - Processing range 2017-01-29 to 2018-02-25\n",
      "  - - Processing range 2017-02-26 to 2018-03-25\n",
      "  - - Processing range 2017-03-26 to 2018-04-22\n",
      "  - - Processing range 2017-04-23 to 2018-05-20\n",
      "  - - Processing range 2017-05-21 to 2018-06-17\n",
      "  - - Processing range 2017-06-18 to 2018-07-15\n",
      "  - - Processing range 2017-07-16 to 2018-08-12\n",
      "Processing product category: housewares\n",
      "  - - Processing range 2017-01-01 to 2018-01-28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\feras\\Anaconda3\\envs\\ForecastingRetailSales\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py:538: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  tensor_proto.tensor_content = nparray.tostring()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - - Processing range 2017-01-29 to 2018-02-25\n",
      "  - - Processing range 2017-02-26 to 2018-03-25\n",
      "  - - Processing range 2017-03-26 to 2018-04-22\n",
      "  - - Processing range 2017-04-23 to 2018-05-20\n",
      "  - - Processing range 2017-05-21 to 2018-06-17\n",
      "  - - Processing range 2017-06-18 to 2018-07-15\n",
      "  - - Processing range 2017-07-16 to 2018-08-12\n",
      "Processing product category: watches_gifts\n",
      "  - - Processing range 2017-01-01 to 2018-01-28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\feras\\Anaconda3\\envs\\ForecastingRetailSales\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py:538: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  tensor_proto.tensor_content = nparray.tostring()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - - Processing range 2017-01-29 to 2018-02-25\n",
      "  - - Processing range 2017-02-26 to 2018-03-25\n",
      "  - - Processing range 2017-03-26 to 2018-04-22\n",
      "  - - Processing range 2017-04-23 to 2018-05-20\n",
      "  - - Processing range 2017-05-21 to 2018-06-17\n",
      "  - - Processing range 2017-06-18 to 2018-07-15\n",
      "  - - Processing range 2017-07-16 to 2018-08-12\n",
      "Processing product category: telephony\n",
      "  - - Processing range 2017-01-01 to 2018-01-28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\feras\\Anaconda3\\envs\\ForecastingRetailSales\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py:538: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  tensor_proto.tensor_content = nparray.tostring()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - - Processing range 2017-01-29 to 2018-02-25\n",
      "  - - Processing range 2017-02-26 to 2018-03-25\n",
      "  - - Processing range 2017-03-26 to 2018-04-22\n",
      "  - - Processing range 2017-04-23 to 2018-05-20\n",
      "  - - Processing range 2017-05-21 to 2018-06-17\n",
      "  - - Processing range 2017-06-18 to 2018-07-15\n",
      "  - - Processing range 2017-07-16 to 2018-08-12\n",
      "Wall time: 10h 6min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Step 1: Read data and convert string to proper datetime objects\n",
    "merged_data = pd.read_csv(os.path.join(proj_path, \n",
    "                                       catalog['output_dir']['dir'], \n",
    "                                       catalog['output_dir']['transactions']))\n",
    "\n",
    "merged_data['order_approved_at'] = pd.to_datetime(merged_data['order_approved_at'])\n",
    "\n",
    "# Step2: Create date folds\n",
    "date_ranges = make_dates(params['olist']['experiment_dates'])\n",
    "\n",
    "\n",
    "for prod_cat in params['olist']['product_categories']:\n",
    "    print(f'Processing product category: {prod_cat}')\n",
    "    \n",
    "    # Initialize mlflow tracking\n",
    "    create_folder(os.path.join(proj_path, 'mlruns'))\n",
    "    #mlflow.set_tracking_uri(os.path.join(proj_path, 'mlruns'))\n",
    "    mlflow.set_tracking_uri(os.path.join('../../','mlruns'))\n",
    "    mlflow.set_experiment(prod_cat)\n",
    "    \n",
    "    start_timer = time()\n",
    "    all_predictions = []\n",
    "    all_y_true = []\n",
    "    all_hyperparameters = []\n",
    "    # Iterate over each period, unpack tuple in each variable.\n",
    "    # in each of the period, we will find the best set of parameters,\n",
    "    # which will represent the time-series cross validation methodology\n",
    "    for _, train_start, train_end, valid_start, valid_end, test_start, test_end in date_ranges.itertuples():\n",
    "        print(f'  - - Processing range {str(train_start.date())} to {str(test_end.date())}')\n",
    "\n",
    "        # This is the only different between experiment 2 and 3\n",
    "#         list_features = ['UNITS','VISITS','HHS','FEATURE','DISPLAY']\n",
    "        list_features = ['payment_value']\n",
    "\n",
    "        # Filter data here\n",
    "        # Prepare the dataset for one UPC and one store, currently no-scaling\n",
    "        filtered_data = merged_data[(merged_data['product_category_name']==prod_cat)][['payment_value','order_approved_at']].copy()\n",
    "        \n",
    "        # Initialize some of the parameters\n",
    "        ws = params['lstm']['window_size']\n",
    "        \n",
    "        # Prepare data, pass in all of the data, it will filter it\n",
    "        data, label, dates = prepare_data(df=filtered_data, \n",
    "                                          lag_units=ws, \n",
    "                                          list_features=list_features)\n",
    "        \n",
    "        # Because we are working with numpy now, it's easier to select the indexes\n",
    "        # Will need date indexes \n",
    "        date_series = pd.Series(dates)\n",
    "        \n",
    "        train_series_idx = date_series[(date_series>=train_start) &\n",
    "                                       (date_series<=train_end)].index\n",
    "        valid_series_idx = date_series[(date_series>=valid_start) &\n",
    "                                       (date_series<=valid_end)].index\n",
    "        test_series_idx = date_series[(date_series>=test_start) &\n",
    "                                      (date_series<=test_end)].index\n",
    "        \n",
    "        train_data = data[train_series_idx].copy()\n",
    "        train_label = label[train_series_idx].copy()\n",
    "        valid_data = data[valid_series_idx].copy()\n",
    "        valid_label = label[valid_series_idx].copy()\n",
    "        test_data = data[test_series_idx].copy()\n",
    "        test_label = label[test_series_idx].copy()\n",
    "        \n",
    "        # The scaling of 0 to 1 creates issue with the loss function, as the minimum will be 0 and maximum 1. \n",
    "        # a bad solution was to drop the row that has 0, but doesn't seem to help. \n",
    "        # another solution is to log the values\n",
    "        \n",
    "        scalers = {}\n",
    "        # The last two features are already bounded 0 to 1, we subtract 2 from the shape which represent 'FEATURE', 'DISPLAY'\n",
    "        # for feature_col in range(train_data.shape[-1]-2):\n",
    "        for feature_col in range(train_data.shape[-1]):\n",
    "            scalers[feature_col] = NormalizeScalerDf()\n",
    "            train_data[:,:,feature_col] = scalers[feature_col].fit_transform(train_data[:,:,feature_col])\n",
    "            valid_data[:,:,feature_col] = scalers[feature_col].transform(valid_data[:,:,feature_col])\n",
    "            test_data[:,:,feature_col] = scalers[feature_col].transform(test_data[:,:,feature_col])\n",
    "        scalers['label'] = NormalizeScalerDf()\n",
    "        train_label = scalers['label'].fit_transform(train_label)\n",
    "        valid_label = scalers['label'].transform(valid_label)\n",
    "        test_label = scalers['label'].transform(test_label)\n",
    "\n",
    "        # Replace NaN's with 0's which are caused by the lagged values. This was done after the scaling to avoid issues\n",
    "        train_data = np.nan_to_num(train_data)\n",
    "        valid_data = np.nan_to_num(valid_data)\n",
    "        test_data = np.nan_to_num(test_data)\n",
    "        \n",
    "        # Initialize some of the parameters\n",
    "        dp = params['lstm']['dropout']\n",
    "        units_strategy = params['lstm']['units_strategy']\n",
    "        optimizers = params['lstm']['optimizers']\n",
    "        losses = params['lstm']['loss']\n",
    "        num_series = train_data.shape[2]   \n",
    "\n",
    "        # Step 3: Define a random search for these parameters, for hyperparameter tuning\n",
    "#         random_number_generator = np.random.RandomState(0) \n",
    "        param_grid = {'lr': uniform(loc=0.0001, scale=0.01),\n",
    "                      'init_units': randint(low=5, high= 100),\n",
    "                      'total_layers': randint(low=1, high=2)}\n",
    "        param_list = list(ParameterSampler(param_grid, n_iter=params['lstm']['search_iter'], random_state=3))\n",
    "        \n",
    "        # Perform hyperparameter search\n",
    "        res = []\n",
    "        for param_dict in param_list:\n",
    "            lr = param_dict['lr']\n",
    "            init_units = param_dict['init_units']\n",
    "            tot_lay = param_dict['total_layers']\n",
    "        \n",
    "            # Early Stopping will allow us to trigger when to stop training the model\n",
    "            # The stopping criteria will be when the validation loss doesn't decrease for \n",
    "            # two consecutive steps. This prevents us from overfitting the model. \n",
    "            earlyStop=EarlyStopping(monitor=\"val_loss\", verbose=0, mode='min', patience=2)\n",
    "            # Initialize the model and train\n",
    "            lstm_model = set_model(n_layers=tot_lay, init_units=init_units,\n",
    "                                   n_unit_strategy=units_strategy, dropout_p=dp,\n",
    "                                   num_timesteps=ws, num_series=num_series,\n",
    "                                   lr=lr, optimizer=optimizers, loss=losses)\n",
    "            history = lstm_model.fit(train_data, train_label, validation_data=(valid_data, valid_label), \n",
    "                                     epochs=100, verbose=0, shuffle=False, batch_size=1, callbacks=[earlyStop])\n",
    "\n",
    "            # _ = lstm_model.predict(np.expand_dims(valid_data[0],axis=0))[0]\n",
    "            # There are many warnings when making predictions, thus I use a comprehension loop\n",
    "            # val_predictions = np.array([lstm_model.predict(np.expand_dims(dd,axis=0))[0] for dd in valid_data])\n",
    "            val_predictions = lstm_model.predict(valid_data)\n",
    "            \n",
    "            y_pred = scalers['label'].inverse_transform(val_predictions)\n",
    "            y_true = scalers['label'].inverse_transform(valid_label)\n",
    "\n",
    "            val_mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "\n",
    "            param_res = {'learning_rate':lr,\n",
    "                         'init_units':init_units,\n",
    "                         'n_layers':tot_lay,\n",
    "                         'val_mape':val_mape}\n",
    "\n",
    "            res.append(param_res)\n",
    "        \n",
    "        # Select the optimal hyper-parameters\n",
    "        best_params = pd.DataFrame(res).sort_values(by='val_mape', ascending=True).iloc[0]\n",
    "        \n",
    "        lstm_model = set_model(n_layers=int(best_params['n_layers']),\n",
    "                               init_units=best_params['init_units'],\n",
    "                               n_unit_strategy=units_strategy,\n",
    "                               dropout_p=dp,\n",
    "                               num_timesteps=ws,\n",
    "                               num_series=num_series,\n",
    "                               lr=best_params['learning_rate'],\n",
    "                               optimizer=optimizers,\n",
    "                               loss=losses)\n",
    "\n",
    "        earlyStop=EarlyStopping(monitor=\"val_loss\", verbose=0, mode='min', patience=2)\n",
    "        history = lstm_model.fit(train_data, train_label, validation_data=(valid_data, valid_label),\n",
    "                                 epochs=100, verbose=0, shuffle=False, batch_size=1, callbacks=[earlyStop])\n",
    "\n",
    "        test_predictions = lstm_model.predict(test_data) \n",
    "        \n",
    "        y_pred = scalers['label'].inverse_transform(test_predictions)\n",
    "        y_true = scalers['label'].inverse_transform(test_label)\n",
    "        \n",
    "        all_y_true.extend(y_true)\n",
    "        all_predictions.extend(y_pred)\n",
    "        all_hyperparameters.append(best_params)\n",
    "        \n",
    "    df_filtered = merged_data[(merged_data['product_category_name'] == prod_cat) &\n",
    "                              (merged_data['order_approved_at'] >= params['olist']['experiment_dates']['test_start']) & \n",
    "                              (merged_data['order_approved_at'] <= params['olist']['experiment_dates']['test_end'])].copy()\n",
    "    \n",
    "    save_data = pd.DataFrame({'y_true': all_y_true,\n",
    "                              'y_pred': np.array(all_predictions).flatten(),\n",
    "                              'dates': df_filtered['order_approved_at']})\n",
    "        \n",
    "    test_metrics = get_metrics(all_y_true, all_predictions)\n",
    "\n",
    "    fdir = os.path.join(proj_path, catalog['results']['dir'])\n",
    "    fname = os.path.join(fdir, f'lstm_exp1_{prod_cat}.csv')\n",
    "    create_folder(fdir)\n",
    "    save_data.to_csv(fname)\n",
    "    \n",
    "    fdir_hparam = os.path.join(proj_path, catalog['extra']['dir'], catalog['extra']['lstm_hyperparams'])\n",
    "    fname_hparam = os.path.join(fdir_hparam, f'lstm_exp1_{prod_cat}.pickle')\n",
    "    create_folder(fdir_hparam)\n",
    "    \n",
    "    with open(fname_hparam, \"wb\") as filehandler:\n",
    "        pickle.dump(all_hyperparameters, filehandler)\n",
    "\n",
    "    duration_min = int((time() - start_timer) // 60)\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_param('model','lstm')\n",
    "        mlflow.log_param('experiment','exp1')\n",
    "        mlflow.log_params(params['lstm'])\n",
    "        mlflow.log_metrics(test_metrics)\n",
    "        mlflow.log_artifact(fname)\n",
    "        mlflow.log_metric('time', duration_min)\n",
    "        mlflow.log_artifact(fname_hparam)\n",
    "        mlflow.log_params({'g_cat_state':False,\n",
    "                           'g_cat_br':False,\n",
    "                           'g_cc_state':False,\n",
    "                           'g_cc_br':False})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
