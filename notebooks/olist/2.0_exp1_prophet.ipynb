{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline II: Facebook Prophet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta, datetime\n",
    "import itertools\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from time import time \n",
    "import mlflow\n",
    "import yaml\n",
    "\n",
    "# Get the current project path (where you open the notebook)\n",
    "# and go up two levels to get the project path\n",
    "current_dir = Path.cwd()\n",
    "proj_path = current_dir.parent.parent\n",
    "\n",
    "# make the code in src available to import in this notebook\n",
    "import sys\n",
    "sys.path.append(os.path.join(proj_path,'src'))\n",
    "\n",
    "from fbprophet import Prophet\n",
    "\n",
    "from metrics import *\n",
    "from utils import *\n",
    "\n",
    "# Catalog contains all the paths related to datasets\n",
    "with open(os.path.join(proj_path, 'conf/catalog.yml'), \"r\") as f:\n",
    "    catalog = yaml.safe_load(f)['olist']\n",
    "    \n",
    "# Params contains all of the dataset creation parameters and model parameters\n",
    "with open(os.path.join(proj_path, 'conf/params.yml'), \"r\") as f:\n",
    "    params = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\feras\\Anaconda3\\envs\\ForecastingRetailSales\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing product category: bed_bath_table\n",
      "Processing range 2017-01-01 to 2018-01-28\n",
      "Processing range 2017-01-29 to 2018-02-25\n",
      "Processing range 2017-02-26 to 2018-03-25\n",
      "Processing range 2017-03-26 to 2018-04-22\n",
      "Processing range 2017-04-23 to 2018-05-20\n",
      "Processing range 2017-05-21 to 2018-06-17\n",
      "Processing range 2017-06-18 to 2018-07-15\n",
      "Processing range 2017-07-16 to 2018-08-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020/12/11 11:04:15 WARNING mlflow.tracking.context.git_context: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "The git executable must be specified in one of the following ways:\n",
      "    - be included in your $PATH\n",
      "    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "    - explicitly set via git.refresh()\n",
      "\n",
      "All git commands will error until this is rectified.\n",
      "\n",
      "This initial warning can be silenced or aggravated in the future by setting the\n",
      "$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "    - quiet|q|silence|s|none|n|0: for no warning or exception\n",
      "    - warn|w|warning|1: for a printed warning\n",
      "    - error|e|raise|r|2: for a raised exception\n",
      "\n",
      "Example:\n",
      "    export GIT_PYTHON_REFRESH=quiet\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing product category: health_beauty\n",
      "Processing range 2017-01-01 to 2018-01-28\n",
      "Processing range 2017-01-29 to 2018-02-25\n",
      "Processing range 2017-02-26 to 2018-03-25\n",
      "Processing range 2017-03-26 to 2018-04-22\n",
      "Processing range 2017-04-23 to 2018-05-20\n",
      "Processing range 2017-05-21 to 2018-06-17\n",
      "Processing range 2017-06-18 to 2018-07-15\n",
      "Processing range 2017-07-16 to 2018-08-12\n",
      "Processing product category: sports_leisure\n",
      "Processing range 2017-01-01 to 2018-01-28\n",
      "Processing range 2017-01-29 to 2018-02-25\n",
      "Processing range 2017-02-26 to 2018-03-25\n",
      "Processing range 2017-03-26 to 2018-04-22\n",
      "Processing range 2017-04-23 to 2018-05-20\n",
      "Processing range 2017-05-21 to 2018-06-17\n",
      "Processing range 2017-06-18 to 2018-07-15\n",
      "Processing range 2017-07-16 to 2018-08-12\n",
      "Processing product category: furniture_decor\n",
      "Processing range 2017-01-01 to 2018-01-28\n",
      "Processing range 2017-01-29 to 2018-02-25\n",
      "Processing range 2017-02-26 to 2018-03-25\n",
      "Processing range 2017-03-26 to 2018-04-22\n",
      "Processing range 2017-04-23 to 2018-05-20\n",
      "Processing range 2017-05-21 to 2018-06-17\n",
      "Processing range 2017-06-18 to 2018-07-15\n",
      "Processing range 2017-07-16 to 2018-08-12\n",
      "Processing product category: housewares\n",
      "Processing range 2017-01-01 to 2018-01-28\n",
      "Processing range 2017-01-29 to 2018-02-25\n",
      "Processing range 2017-02-26 to 2018-03-25\n",
      "Processing range 2017-03-26 to 2018-04-22\n",
      "Processing range 2017-04-23 to 2018-05-20\n",
      "Processing range 2017-05-21 to 2018-06-17\n",
      "Processing range 2017-06-18 to 2018-07-15\n",
      "Processing range 2017-07-16 to 2018-08-12\n",
      "Processing product category: watches_gifts\n",
      "Processing range 2017-01-01 to 2018-01-28\n",
      "Processing range 2017-01-29 to 2018-02-25\n",
      "Processing range 2017-02-26 to 2018-03-25\n",
      "Processing range 2017-03-26 to 2018-04-22\n",
      "Processing range 2017-04-23 to 2018-05-20\n",
      "Processing range 2017-05-21 to 2018-06-17\n",
      "Processing range 2017-06-18 to 2018-07-15\n",
      "Processing range 2017-07-16 to 2018-08-12\n",
      "Processing product category: telephony\n",
      "Processing range 2017-01-01 to 2018-01-28\n",
      "Processing range 2017-01-29 to 2018-02-25\n",
      "Processing range 2017-02-26 to 2018-03-25\n",
      "Processing range 2017-03-26 to 2018-04-22\n",
      "Processing range 2017-04-23 to 2018-05-20\n",
      "Processing range 2017-05-21 to 2018-06-17\n",
      "Processing range 2017-06-18 to 2018-07-15\n",
      "Processing range 2017-07-16 to 2018-08-12\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Read data\n",
    "merged_data = pd.read_csv(os.path.join(proj_path, \n",
    "                                       catalog['output_dir']['dir'], \n",
    "                                       catalog['output_dir']['transactions']))\n",
    "\n",
    "merged_data['order_approved_at'] = pd.to_datetime(merged_data['order_approved_at'])\n",
    "# merged_data['order_approved_at'] = merged_data['order_approved_at'] + timedelta(days=3)\n",
    "\n",
    "# Step2: Create date folds\n",
    "date_ranges = make_dates(params['olist']['experiment_dates'])\n",
    "\n",
    "for prod_cat in params['olist']['product_categories']:\n",
    "    print(f'Processing product category: {prod_cat}')\n",
    "    \n",
    "    # Initialize mlflow tracking\n",
    "    create_folder(os.path.join(proj_path, 'mlruns'))\n",
    "    #mlflow.set_tracking_uri(os.path.join(proj_path, 'mlruns'))\n",
    "    mlflow.set_tracking_uri(os.path.join('../../','mlruns'))\n",
    "    mlflow.set_experiment(prod_cat)\n",
    "    \n",
    "    start_timer = time()\n",
    "    all_predictions = []\n",
    "    # Iterate over each period, unpack tuple in each variable.\n",
    "    # in each of the period, we will find the best set of parameters,\n",
    "    # which will represent the time-series cross validation methodology\n",
    "    for _, train_start, train_end, valid_start, valid_end, test_start, test_end in date_ranges.itertuples():\n",
    "        print(f'Processing range {str(train_start.date())} to {str(test_end.date())}')\n",
    "\n",
    "        train_x = merged_data[(merged_data['order_approved_at']>=train_start) &\n",
    "                                  (merged_data['order_approved_at']<=valid_end) &\n",
    "                                  (merged_data['product_category_name']==prod_cat)][['order_approved_at','payment_value']]\n",
    "        \n",
    "        # Doesn't need a validation period.\n",
    "        test_y = merged_data[(merged_data['order_approved_at']>=test_start) &\n",
    "                             (merged_data['order_approved_at']<=test_end) &\n",
    "                             (merged_data['product_category_name']==prod_cat)][['order_approved_at','payment_value']]\n",
    "        # Prophet expects two columns, one with the label 'ds' for the dates and y for the values\n",
    "        train_x = train_x.rename(columns={'order_approved_at':'ds', 'payment_value':'y'})\n",
    "        test_y = test_y.rename(columns={'order_approved_at':'ds', 'payment_value':'y'})\n",
    "\n",
    "        # Iterate over the periods to make next-day forecasts\n",
    "        predictions = []\n",
    "        for i in range(test_y.shape[0]):\n",
    "\n",
    "            #Instantiate a new Prophet object that represents the model\n",
    "            model = Prophet(weekly_seasonality=True,\n",
    "                            yearly_seasonality=True,\n",
    "                            daily_seasonality=False)\n",
    "\n",
    "            #Call the built-in holiday collection for US to be included in the model\n",
    "            model.add_country_holidays(country_name='BR')\n",
    "\n",
    "            # Fit the FB Prohpet Model\n",
    "            model.fit(pd.concat([train_x.iloc[i:], test_y.iloc[:i]]))\n",
    "            future = model.make_future_dataframe(periods=1, freq='7D')\n",
    "            fcst = model.predict(future)['yhat'].iloc[-1]\n",
    "            predictions.append(fcst)\n",
    "        \n",
    "        all_predictions.extend(predictions)\n",
    "    \n",
    "    df_filtered = merged_data[(merged_data['product_category_name'] == prod_cat) &\n",
    "                              (merged_data['order_approved_at'] >= params['olist']['experiment_dates']['test_start']) & \n",
    "                              (merged_data['order_approved_at'] <= params['olist']['experiment_dates']['test_end'])].copy()\n",
    "     \n",
    "    metrics = get_metrics(df_filtered['payment_value'].values, all_predictions)\n",
    "\n",
    "    # store predictions\n",
    "    fdir = os.path.join(proj_path, catalog['results']['dir'])\n",
    "    fname = os.path.join(fdir, f'exp1_prophet_{prod_cat}.csv')\n",
    "    create_folder(fdir)\n",
    "\n",
    "    save_data = pd.DataFrame({'y_true': df_filtered['payment_value'].values,\n",
    "                              'preds': all_predictions,\n",
    "                              'dates': df_filtered['order_approved_at']})\n",
    "\n",
    "    save_data.to_csv(fname)\n",
    "    duration_min = int((time() - start_timer) // 60)\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_artifact(fname)\n",
    "        mlflow.log_param('Product Category',prod_cat)\n",
    "        mlflow.log_param('model','prophet')\n",
    "        mlflow.log_metrics(metrics)\n",
    "        mlflow.log_metric('time', duration_min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
