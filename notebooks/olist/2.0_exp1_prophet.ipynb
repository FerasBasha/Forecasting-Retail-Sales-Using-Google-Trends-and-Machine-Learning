{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline II: Facebook Prophet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta, datetime\n",
    "import itertools\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from time import time \n",
    "import mlflow\n",
    "import yaml\n",
    "\n",
    "# Get the current project path (where you open the notebook)\n",
    "# and go up two levels to get the project path\n",
    "current_dir = Path.cwd()\n",
    "proj_path = current_dir.parent.parent\n",
    "\n",
    "# make the code in src available to import in this notebook\n",
    "import sys\n",
    "sys.path.append(os.path.join(proj_path,'src'))\n",
    "\n",
    "from fbprophet import Prophet\n",
    "\n",
    "from metrics import *\n",
    "from utils import *\n",
    "\n",
    "# Catalog contains all the paths related to datasets\n",
    "with open(os.path.join(proj_path, 'conf/catalog.yml'), \"r\") as f:\n",
    "    catalog = yaml.safe_load(f)['olist']\n",
    "    \n",
    "# Params contains all of the dataset creation parameters and model parameters\n",
    "with open(os.path.join(proj_path, 'conf/params.yml'), \"r\") as f:\n",
    "    params = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing product category: bed_bath_table\n",
      "Processing range 2017-01-01 to 2018-01-28\n",
      "Processing range 2017-01-29 to 2018-02-25\n",
      "Processing range 2017-02-26 to 2018-03-25\n",
      "Processing range 2017-03-26 to 2018-04-22\n",
      "Processing range 2017-04-23 to 2018-05-20\n",
      "Processing range 2017-05-21 to 2018-06-17\n",
      "Processing range 2017-06-18 to 2018-07-15\n",
      "Processing range 2017-07-16 to 2018-08-12\n",
      "Processing product category: health_beauty\n",
      "Processing range 2017-01-01 to 2018-01-28\n",
      "Processing range 2017-01-29 to 2018-02-25\n",
      "Processing range 2017-02-26 to 2018-03-25\n",
      "Processing range 2017-03-26 to 2018-04-22\n",
      "Processing range 2017-04-23 to 2018-05-20\n",
      "Processing range 2017-05-21 to 2018-06-17\n",
      "Processing range 2017-06-18 to 2018-07-15\n",
      "Processing range 2017-07-16 to 2018-08-12\n",
      "Processing product category: sports_leisure\n",
      "Processing range 2017-01-01 to 2018-01-28\n",
      "Processing range 2017-01-29 to 2018-02-25\n",
      "Processing range 2017-02-26 to 2018-03-25\n",
      "Processing range 2017-03-26 to 2018-04-22\n",
      "Processing range 2017-04-23 to 2018-05-20\n",
      "Processing range 2017-05-21 to 2018-06-17\n",
      "Processing range 2017-06-18 to 2018-07-15\n",
      "Processing range 2017-07-16 to 2018-08-12\n",
      "Processing product category: furniture_decor\n",
      "Processing range 2017-01-01 to 2018-01-28\n",
      "Processing range 2017-01-29 to 2018-02-25\n",
      "Processing range 2017-02-26 to 2018-03-25\n",
      "Processing range 2017-03-26 to 2018-04-22\n",
      "Processing range 2017-04-23 to 2018-05-20\n",
      "Processing range 2017-05-21 to 2018-06-17\n",
      "Processing range 2017-06-18 to 2018-07-15\n",
      "Processing range 2017-07-16 to 2018-08-12\n",
      "Processing product category: housewares\n",
      "Processing range 2017-01-01 to 2018-01-28\n",
      "Processing range 2017-01-29 to 2018-02-25\n",
      "Processing range 2017-02-26 to 2018-03-25\n",
      "Processing range 2017-03-26 to 2018-04-22\n",
      "Processing range 2017-04-23 to 2018-05-20\n",
      "Processing range 2017-05-21 to 2018-06-17\n",
      "Processing range 2017-06-18 to 2018-07-15\n",
      "Processing range 2017-07-16 to 2018-08-12\n",
      "Processing product category: watches_gifts\n",
      "Processing range 2017-01-01 to 2018-01-28\n",
      "Processing range 2017-01-29 to 2018-02-25\n",
      "Processing range 2017-02-26 to 2018-03-25\n",
      "Processing range 2017-03-26 to 2018-04-22\n",
      "Processing range 2017-04-23 to 2018-05-20\n",
      "Processing range 2017-05-21 to 2018-06-17\n",
      "Processing range 2017-06-18 to 2018-07-15\n",
      "Processing range 2017-07-16 to 2018-08-12\n",
      "Processing product category: telephony\n",
      "Processing range 2017-01-01 to 2018-01-28\n",
      "Processing range 2017-01-29 to 2018-02-25\n",
      "Processing range 2017-02-26 to 2018-03-25\n",
      "Processing range 2017-03-26 to 2018-04-22\n",
      "Processing range 2017-04-23 to 2018-05-20\n",
      "Processing range 2017-05-21 to 2018-06-17\n",
      "Processing range 2017-06-18 to 2018-07-15\n",
      "Processing range 2017-07-16 to 2018-08-12\n",
      "Processing product category: garden_tools\n",
      "Processing range 2017-01-01 to 2018-01-28\n",
      "Processing range 2017-01-29 to 2018-02-25\n",
      "Processing range 2017-02-26 to 2018-03-25\n",
      "Processing range 2017-03-26 to 2018-04-22\n",
      "Processing range 2017-04-23 to 2018-05-20\n",
      "Processing range 2017-05-21 to 2018-06-17\n",
      "Processing range 2017-06-18 to 2018-07-15\n",
      "Processing range 2017-07-16 to 2018-08-12\n",
      "Processing product category: auto\n",
      "Processing range 2017-01-01 to 2018-01-28\n",
      "Processing range 2017-01-29 to 2018-02-25\n",
      "Processing range 2017-02-26 to 2018-03-25\n",
      "Processing range 2017-03-26 to 2018-04-22\n",
      "Processing range 2017-04-23 to 2018-05-20\n",
      "Processing range 2017-05-21 to 2018-06-17\n",
      "Processing range 2017-06-18 to 2018-07-15\n",
      "Processing range 2017-07-16 to 2018-08-12\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Read data\n",
    "merged_data = pd.read_csv(os.path.join(proj_path, \n",
    "                                       catalog['output_dir']['dir'], \n",
    "                                       catalog['output_dir']['transactions']))\n",
    "\n",
    "merged_data['order_approved_at'] = pd.to_datetime(merged_data['order_approved_at'])\n",
    "# merged_data['order_approved_at'] = merged_data['order_approved_at'] + timedelta(days=3)\n",
    "\n",
    "# Step2: Create date folds\n",
    "date_ranges = make_dates(params['olist']['experiment_dates'])\n",
    "\n",
    "for prod_cat in params['olist']['product_categories']:\n",
    "    print(f'Processing product category: {prod_cat}')\n",
    "    \n",
    "    # Initialize mlflow tracking\n",
    "    create_folder(os.path.join(proj_path, 'mlruns'))\n",
    "    mlflow.set_tracking_uri(os.path.join(proj_path, 'mlruns'))\n",
    "    mlflow.set_experiment(prod_cat)\n",
    "    \n",
    "    start_timer = time()\n",
    "    all_predictions = []\n",
    "    # Iterate over each period, unpack tuple in each variable.\n",
    "    # in each of the period, we will find the best set of parameters,\n",
    "    # which will represent the time-series cross validation methodology\n",
    "    for _, train_start, train_end, valid_start, valid_end, test_start, test_end in date_ranges.itertuples():\n",
    "        print(f'Processing range {str(train_start.date())} to {str(test_end.date())}')\n",
    "\n",
    "        train_x = merged_data[(merged_data['order_approved_at']>=train_start) &\n",
    "                                  (merged_data['order_approved_at']<=valid_end) &\n",
    "                                  (merged_data['product_category_name']==prod_cat)][['order_approved_at','payment_value']]\n",
    "        \n",
    "        # Doesn't need a validation period.\n",
    "        test_y = merged_data[(merged_data['order_approved_at']>=test_start) &\n",
    "                             (merged_data['order_approved_at']<=test_end) &\n",
    "                             (merged_data['product_category_name']==prod_cat)][['order_approved_at','payment_value']]\n",
    "        # Prophet expects two columns, one with the label 'ds' for the dates and y for the values\n",
    "        train_x = train_x.rename(columns={'order_approved_at':'ds', 'payment_value':'y'})\n",
    "        test_y = test_y.rename(columns={'order_approved_at':'ds', 'payment_value':'y'})\n",
    "\n",
    "        # Iterate over the periods to make next-day forecasts\n",
    "        predictions = []\n",
    "        for i in range(test_y.shape[0]):\n",
    "\n",
    "            #Instantiate a new Prophet object that represents the model\n",
    "            model = Prophet(weekly_seasonality=True,\n",
    "                            yearly_seasonality=True,\n",
    "                            daily_seasonality=False)\n",
    "\n",
    "            #Call the built-in holiday collection for US to be included in the model\n",
    "            model.add_country_holidays(country_name='BR')\n",
    "\n",
    "            # Fit the FB Prohpet Model\n",
    "            model.fit(pd.concat([train_x.iloc[i:], test_y.iloc[:i]]))\n",
    "            future = model.make_future_dataframe(periods=1, freq='7D')\n",
    "            fcst = model.predict(future)['yhat'].iloc[-1]\n",
    "            predictions.append(fcst)\n",
    "        \n",
    "        all_predictions.extend(predictions)\n",
    "    \n",
    "    df_filtered = merged_data[(merged_data['product_category_name'] == prod_cat) &\n",
    "                              (merged_data['order_approved_at'] >= params['olist']['experiment_dates']['test_start']) & \n",
    "                              (merged_data['order_approved_at'] <= params['olist']['experiment_dates']['test_end'])].copy()\n",
    "     \n",
    "    metrics = get_metrics(df_filtered['payment_value'].values, all_predictions)\n",
    "\n",
    "    # store predictions\n",
    "    fdir = os.path.join(proj_path, catalog['results']['dir'])\n",
    "    fname = os.path.join(fdir, f'exp1_prophet_{prod_cat}.csv')\n",
    "    create_folder(fdir)\n",
    "\n",
    "    save_data = pd.DataFrame({'y_true': df_filtered['payment_value'].values,\n",
    "                              'preds': all_predictions,\n",
    "                              'dates': df_filtered['order_approved_at']})\n",
    "\n",
    "    save_data.to_csv(fname)\n",
    "    duration_min = int((time() - start_timer) // 60)\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_artifact(fname)\n",
    "        mlflow.log_param('Product Category',prod_cat)\n",
    "        mlflow.log_param('model','prophet')\n",
    "        mlflow.log_metrics(metrics)\n",
    "        mlflow.log_metric('time', duration_min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
