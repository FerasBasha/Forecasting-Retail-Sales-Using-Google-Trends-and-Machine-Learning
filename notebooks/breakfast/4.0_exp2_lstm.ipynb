{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('CRITICAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda remove -n olist3 tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall -y tensorflow\n",
    "# !pip install tensorflow==2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import timedelta, datetime\n",
    "import itertools\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import yaml\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK, plotting, space_eval\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from scipy.stats.distributions import uniform, randint\n",
    "\n",
    "from glob import glob\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "\n",
    "# Get the current project path (where you open the notebook)\n",
    "# and go up two levels to get the project path\n",
    "current_dir = Path.cwd()\n",
    "proj_path = current_dir.parent.parent\n",
    "\n",
    "\n",
    "# make the code in src available to import in this notebook\n",
    "import sys\n",
    "sys.path.append(os.path.join(proj_path,'src'))\n",
    "\n",
    "from metrics import *\n",
    "from utils import *\n",
    "from scalers import *\n",
    "\n",
    "# Catalog contains all the paths related to datasets\n",
    "with open(os.path.join(proj_path, 'conf/catalog.yml'), \"r\") as f:\n",
    "    catalog = yaml.safe_load(f)['breakfast']\n",
    "    \n",
    "# Params contains all of the dataset creation parameters and model parameters\n",
    "with open(os.path.join(proj_path, 'conf/params.yml'), \"r\") as f:\n",
    "    params = yaml.safe_load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model(n_layers:int=1,init_units:int=50,n_unit_strategy:str='stable', \n",
    "              dropout_p:float=0.1, num_timesteps: int=8, num_series: int=5, \n",
    "              lr:float=0.001,optimizer:str='adam', loss:str='mape'):\n",
    "    '''\n",
    "    # Gives flexibility\n",
    "    # Explore depth vs width of the RNN model\n",
    "    https://stackoverflow.com/questions/59072728/what-is-the-rule-to-know-how-many-lstm-cells-and-how-many-units-in-each-lstm-cel\n",
    "    # there are no rule of thumb, but here the decrease strategy will devide by i the number of units at each layer.\n",
    "    \n",
    "    Args\n",
    "        num_timesteps: the number of lags in the dataframes\n",
    "        num_series: the number of time series, specify 1 if univariate\n",
    "        n_unit_strategy: two options, stable and decrease. decrease will decrease the  \n",
    "    '''\n",
    "    model = Sequential()\n",
    "    if n_unit_strategy == 'stable':\n",
    "        n_units = [init_units] * n_layers\n",
    "    if n_unit_strategy == 'decrease':\n",
    "        n_units = [max(int(init_units/i),1) for i in range(1,n_layers+1)]\n",
    "    \n",
    "    if n_layers > 1:\n",
    "        model.add(LSTM(n_units[0], return_sequences=True, input_shape=(num_timesteps, num_series)))\n",
    "        model.add(Dropout(dropout_p)) # Prevent overfitting\n",
    "        for i in range(1, n_layers):\n",
    "            model.add(LSTM(n_units[i], return_sequences=True))\n",
    "            model.add(Dropout(dropout_p)) \n",
    "        model.add(LSTM(n_units[-1]))\n",
    "        model.add(Dropout(dropout_p))\n",
    "    \n",
    "    else:\n",
    "        model.add(LSTM(n_units[0], return_sequences=False, input_shape=(num_timesteps, num_series)))\n",
    "        model.add(Dropout(dropout_p)) # Prevent overfitting\n",
    "\n",
    "    model.add(Dense(units=1))\n",
    "    \n",
    "    if optimizer == 'adam':\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    if optimizer == 'rmsprop':\n",
    "        opt = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "    \n",
    "    model.compile(optimizer=opt, loss=loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, lag_units, list_features:list=['UNITS'], g_features:list=[], lag_g_units:int=4):\n",
    "    \"\"\"\n",
    "    For every feature in the list of features, create lagged features\n",
    "    for a specified number of lagged units. The lenght must be the\n",
    "    same for all of the features. After creating the new features,\n",
    "    it filters on those and adds them to a list that will be \n",
    "    reshaped in the correct format.\n",
    "    \n",
    "    We go from a dataframe version:\n",
    "    \n",
    "    UNITS    | FEATURE_A | FEATURE_B |   ...\n",
    "    ---------|-----------|-----------|-------\n",
    "    \n",
    "    For every feature:\n",
    "        \n",
    "        Create the lagged features:\n",
    "\n",
    "        UNITS    | FEATURE_A | FEATURE_A-lag-1 |   ...\n",
    "        ---------|-----------|-----------------|-------  \n",
    "\n",
    "        Filter on each lagged feature and inverse columns:\n",
    "\n",
    "        FEATURE_A-lag-N | ... |  FEATURE_A-lag-1\n",
    "        ----------------|-----|------------------\n",
    "        \n",
    "        Convert to numpy as reshape by adding a dimension\n",
    "        (nrows, nlags) -> (nrows, nlags, 1)\n",
    "        \n",
    "    Concatenate each \"feature-specific matrix\" on the last dimension\n",
    "    \n",
    "    [(nrows, lagged-features-A, 1),\n",
    "     (nrows, lagged-features-B, 1),\n",
    "                  ...\n",
    "     (nrows, lagged-features-X, 1)]\n",
    "     \n",
    "     Results in: \n",
    "     \n",
    "     (nrows, nlags, N)\n",
    "    \n",
    "    \n",
    "    Note: We need to specify the column 'UNITS' in the list of features.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize a list\n",
    "    list_df = []\n",
    "    \n",
    "    for feature in (list_features+g_features):\n",
    "        # create new features that will have the convention lag-<col_name>-<lagged unit>\n",
    "        if feature in list_features:\n",
    "            make_lag_features(df, lag_units, col_name=feature, prefix_name=f'lag-{feature}',inplace=True)\n",
    "        if feature in g_features:\n",
    "            make_lag_features(df, lag_g_units, col_name=feature, prefix_name=f'lag-{feature}',inplace=True)\n",
    "        # filter columns that were just created\n",
    "        # we are only appending lag something, so not actual features\n",
    "        dummy = df.filter(like=f'lag-{feature}')\n",
    "        # invert columns left to right (lag-1 ... lag-N) -> (lag-N ... lag-1)\n",
    "        dummy = dummy[dummy.columns[::-1]]\n",
    "        # add an extra dimension\n",
    "        dummy = dummy.values.reshape(dummy.shape[0], dummy.shape[1], 1)\n",
    "        # add the matrice to a list where it will get concatenated at the end\n",
    "        list_df.append(dummy)\n",
    "        \n",
    "    \n",
    "\n",
    "    # matrices in list_df must have shape (n_rows, n_cols, 1)\n",
    "    # so we can concatenate on the last axis\n",
    "    x_features = np.concatenate(list_df, axis=2)\n",
    "    y_feature = np.array(df['UNITS'].values)\n",
    "\n",
    "    return x_features, y_feature, df['WEEK_END_DATE'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing store 389 upc 7192100339\n",
      "  -Processing range 2009-01-17 to 2011-01-29\n",
      "  --Loading Google Trends data\n",
      "  --Preparing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --Start Hyperparameter search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:37<00:00,  7.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --Done search, training model.\n",
      "  --Making predictions and saving.\n",
      "CPU times: user 3min 55s, sys: 53.2 s, total: 4min 48s\n",
      "Wall time: 2min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# Step 1: Read data\n",
    "merged_data = pd.read_csv(os.path.join(proj_path, catalog['output_dir']['dir'], catalog['output_dir']['merged']))\n",
    "merged_data['WEEK_END_DATE'] = pd.to_datetime(merged_data['WEEK_END_DATE'])\n",
    "merged_data['WEEK_END_DATE'] = merged_data['WEEK_END_DATE'] + timedelta(days=3)\n",
    "\n",
    "# Step 2: Create list of stores and upc pairs\n",
    "stores = list(params['breakfast']['dataset']['store_ids'].keys())\n",
    "upcs = list(params['breakfast']['dataset']['upc_ids'].keys())\n",
    "store_upc_pairs = list(itertools.product(stores, upcs))[7:]\n",
    "\n",
    "# Create date folds\n",
    "date_ranges = make_dates(params['breakfast']['experiment_dates'])\n",
    "\n",
    "for store_id, upc_id in store_upc_pairs: \n",
    "    print(f'Processing store {store_id} upc {upc_id}')\n",
    "    create_folder(os.path.join(proj_path, 'mlruns'))\n",
    "    mlflow.set_tracking_uri(\"../../mlruns\")\n",
    "    mlflow.set_experiment(f'{store_id}_{upc_id}')\n",
    "\n",
    "    # Iterate over each period, unpack tuple in each variable.\n",
    "    # in each of the period, we will find the best set of parameters,\n",
    "    # which will represent the time-series cross validation methodology\n",
    "    for _, train_start, train_end, valid_start, valid_end, test_start, test_end in date_ranges.itertuples():\n",
    "        print(f'  -Processing range {str(train_start.date())} to {str(test_end.date())}')\n",
    "\n",
    "        # This is the only different between experiment 2 and 3\n",
    "#         list_features = ['UNITS','VISITS','HHS','FEATURE','DISPLAY']\n",
    "        list_features = ['UNITS']\n",
    "\n",
    "        # Filter data here\n",
    "        # Prepare the dataset for one UPC and one store, currently no-scaling\n",
    "        filtered_data = merged_data[(merged_data['STORE_NUM']==store_id) &\n",
    "                                     (merged_data['UPC']==upc_id)][list_features+['WEEK_END_DATE']].copy()\n",
    "\n",
    "        print('  --Loading Google Trends data')\n",
    "        # ┌────────────────── GOOGLE TRENDS STARTS HERE ────────────────────┐\n",
    "        # │\n",
    "        # Get product category and state\n",
    "        state = params['breakfast']['dataset']['store_ids'][store_id]\n",
    "        cat = params['breakfast']['dataset']['upc_ids'][upc_id]\n",
    "\n",
    "        # Do we need an epty glist ?\n",
    "        glist = []\n",
    "        # List comprehension to iteratively read in the correct directory the google\n",
    "        # trends csv files that match the appropriate pattern of * (everything) in respective state\n",
    "        # returns a list of pandas dataframes where after we concat on the column axis using the index_col\n",
    "        # After that, we reset the index, to get the Week as a column and convert to datetime\n",
    "        # Finaly, we add 6 days as the google trends hits represents the end of week and corresponds\n",
    "        # to Breakfast At the Frat dates\n",
    "        glist = [\n",
    "            pd.read_csv(path, \n",
    "                        skiprows= 3, \n",
    "                        index_col= 'Week', \n",
    "                        names= ['Week', parse_name([str(proj_path), f'/data/03_external/{cat}/', '.csv'],path)]) \n",
    "            for path in glob(str(proj_path / f'data/03_external/{cat}/*_{state}.csv'))\n",
    "        ]\n",
    "        glist_us =  [\n",
    "            pd.read_csv(path, \n",
    "                        skiprows= 3, \n",
    "                        index_col= 'Week', \n",
    "                        names= ['Week', parse_name([str(proj_path), f'/data/03_external/{cat}/', '.csv'],path)]) \n",
    "            for path in glob(str(proj_path / f'data/03_external/{cat}/*_US.csv'))\n",
    "        ]\n",
    "        # Cross Category\n",
    "        glist_cc_state =  [\n",
    "            pd.read_csv(path, \n",
    "                        skiprows= 3, \n",
    "                        index_col= 'Week', \n",
    "                        names= ['Week', parse_name([str(proj_path), f'/data/03_external/cross_category/', '.csv'],path)]) \n",
    "            for path in glob(str(proj_path / f'data/03_external/cross_category/*_{state}.csv'))\n",
    "        ]\n",
    "        glist_cc_us =  [\n",
    "            pd.read_csv(path, \n",
    "                        skiprows= 3, \n",
    "                        index_col= 'Week', \n",
    "                        names= ['Week', parse_name([str(proj_path), f'/data/03_external/cross_category/', '.csv'],path)]) \n",
    "            for path in glob(str(proj_path / 'data/03_external/cross_category/*_US.csv'))\n",
    "        ]\n",
    "        # This is where you decide on whether to add those series, comment if you do not\n",
    "        # The series should all have unique names, and is based on name of the file\n",
    "        glist.extend(glist_us)\n",
    "        glist.extend(glist_cc_state)\n",
    "        glist.extend(glist_cc_us)\n",
    "\n",
    "        # Combine all of the series into one, column-wise\n",
    "        gdata = pd.concat(glist, axis=1).reset_index()\n",
    "        gdata = fix_col_syntax(gdata)\n",
    "        # reset index will always set the Week as the first column, so select after that date.\n",
    "        gdata_feature_cols = gdata.columns[1:].to_list()\n",
    "        gdata['Week'] = pd.to_datetime(gdata['Week'])\n",
    "        gdata['Week'] = gdata['Week'] + timedelta(days=6)\n",
    "        # Join on filtered_data\n",
    "        filtered_data = filtered_data.merge(gdata, left_on='WEEK_END_DATE', right_on='Week', how='left').drop(columns='Week')\n",
    "        # │ \n",
    "        # └────────────────────── GOOGLE TRENDS ENDS HERE ─────────────────────────┘\n",
    "        \n",
    "        # Initialize some of the parameters\n",
    "        ws = params['lstm']['window_size']\n",
    "\n",
    "        print('  --Preparing data')\n",
    "        # Prepare data, pass in all of the data, it will filter it\n",
    "        data, label, dates = prepare_data(filtered_data, \n",
    "                                          ws, \n",
    "                                          list_features=list_features,\n",
    "                                          g_features=gdata_feature_cols,\n",
    "                                          lag_g_units= params['lstm']['gtrends_window_size'])\n",
    "        \n",
    "        \n",
    "        # Because we are working with numpy now, it's easier to select the indexes\n",
    "        # Will need date indexes \n",
    "        date_series = pd.Series(dates)        \n",
    "        \n",
    "        train_series_idx = date_series[(date_series>=train_start) &\n",
    "                                       (date_series<=train_end)].index\n",
    "        valid_series_idx = date_series[(date_series>=valid_start) &\n",
    "                                       (date_series<=valid_end)].index\n",
    "        test_series_idx = date_series[(date_series>=test_start) &\n",
    "                                      (date_series<=test_end)].index\n",
    "        train_data = data[train_series_idx].copy()\n",
    "        train_label = label[train_series_idx].copy()\n",
    "        valid_data = data[valid_series_idx].copy()\n",
    "        valid_label = label[valid_series_idx].copy()\n",
    "        test_data = data[test_series_idx].copy()\n",
    "        test_label = label[test_series_idx].copy()   \n",
    "        \n",
    "        # The scaling of 0 to 1 creates issue with the loss function, as the minimum will be 0 and maximum 1. \n",
    "        # a bad solution was to drop the row that has 0, but doesn't seem to help. \n",
    "        # another solution is to log the values\n",
    "        \n",
    "        scalers = {}\n",
    "        # The last two features are already bounded 0 to 1, we subtract 2 from the shape which represent 'FEATURE', 'DISPLAY'\n",
    "        # for feature_col in range(train_data.shape[-1]-2):\n",
    "        for feature_col in range(train_data.shape[-1]):\n",
    "            scalers[feature_col] = NormalizeScalerDf()\n",
    "            train_data[:,:,feature_col] = scalers[feature_col].fit_transform(train_data[:,:,feature_col])\n",
    "            valid_data[:,:,feature_col] = scalers[feature_col].transform(valid_data[:,:,feature_col])\n",
    "            test_data[:,:,feature_col] = scalers[feature_col].transform(test_data[:,:,feature_col])\n",
    "        scalers['label'] = NormalizeScalerDf()\n",
    "        train_label = scalers['label'].fit_transform(train_label)\n",
    "        valid_label = scalers['label'].transform(valid_label)\n",
    "        test_label = scalers['label'].transform(test_label)\n",
    "\n",
    "        # Replace NaN's with 0's which are caused by the lagged values. This was done after the scaling to avoid issues\n",
    "        train_data = np.nan_to_num(train_data)\n",
    "        valid_data = np.nan_to_num(valid_data)\n",
    "        test_data = np.nan_to_num(test_data)\n",
    "        \n",
    "        # Initialize some of the parameters\n",
    "        dp = params['lstm']['dropout']\n",
    "        units_strategy = params['lstm']['units_strategy']\n",
    "        optimizers = params['lstm']['optimizers']\n",
    "        losses = params['lstm']['loss']\n",
    "        num_series = train_data.shape[2]   \n",
    "\n",
    "        # Step 3: Define a random search for these parameters, for hyperparameter tuning\n",
    "#         random_number_generator = np.random.RandomState(0) \n",
    "        param_grid = {'lr': uniform(loc=0.001, scale=0.01),\n",
    "                      'init_units': randint(low=5, high= 50),\n",
    "                      'total_layers': randint(low=1, high=2)}\n",
    "        param_list = list(ParameterSampler(param_grid, n_iter=params['lstm']['search_iter'], random_state=3))\n",
    "        \n",
    "        print('  --Start Hyperparameter search')\n",
    "        # Perform hyperparameter search\n",
    "        res = []\n",
    "        for param_dict in tqdm(param_list):\n",
    "            lr = param_dict['lr']\n",
    "            init_units = param_dict['init_units']\n",
    "            tot_lay = param_dict['total_layers']\n",
    "        \n",
    "            # Early Stopping will allow us to trigger when to stop training the model\n",
    "            # The stopping criteria will be when the validation loss doesn't decrease for \n",
    "            # two consecutive steps. This prevents us from overfitting the model. \n",
    "            earlyStop=EarlyStopping(monitor=\"val_loss\", verbose=0, mode='min', patience=2)\n",
    "            # Initialize the model and train\n",
    "            lstm_model = set_model(n_layers=tot_lay, init_units=init_units,\n",
    "                                   n_unit_strategy=units_strategy, dropout_p=dp,\n",
    "                                   num_timesteps=ws, num_series=num_series,\n",
    "                                   lr=lr, optimizer=optimizers, loss=losses)\n",
    "            history = lstm_model.fit(train_data, train_label, validation_data=(valid_data, valid_label), \n",
    "                                     epochs=100, verbose=0, shuffle=False, batch_size=1, callbacks=[earlyStop])\n",
    "\n",
    "            # _ = lstm_model.predict(np.expand_dims(valid_data[0],axis=0))[0]\n",
    "            # There are many warnings when making predictions, thus I use a comprehension loop\n",
    "            # val_predictions = np.array([lstm_model.predict(np.expand_dims(dd,axis=0))[0] for dd in valid_data])\n",
    "            val_predictions = lstm_model.predict(valid_data)\n",
    "            \n",
    "            y_pred = scalers['label'].inverse_transform(val_predictions)\n",
    "            y_true = scalers['label'].inverse_transform(valid_label)\n",
    "\n",
    "            val_mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "\n",
    "            param_res = {'learning_rate':lr,\n",
    "                         'init_units':init_units,\n",
    "                         'n_layers':tot_lay,\n",
    "                         'val_mape':val_mape}\n",
    "\n",
    "            res.append(param_res)\n",
    "        print('  --Done search, training model.')\n",
    "        # Select the optimal hyper-parameters\n",
    "        best_params = pd.DataFrame(res).sort_values(by='val_mape', ascending=True).iloc[0]\n",
    "\n",
    "        lstm_model = set_model(n_layers=int(best_params['n_layers']),\n",
    "                               init_units=best_params['init_units'],\n",
    "                               n_unit_strategy=units_strategy,\n",
    "                               dropout_p=dp,\n",
    "                               num_timesteps=ws,\n",
    "                               num_series=num_series,\n",
    "                               lr=best_params['learning_rate'],\n",
    "                               optimizer=optimizers,\n",
    "                               loss=losses)\n",
    "\n",
    "        earlyStop=EarlyStopping(monitor=\"val_loss\", verbose=0, mode='min', patience=2)\n",
    "        history = lstm_model.fit(train_data, train_label, validation_data=(valid_data, valid_label),\n",
    "                                 epochs=100, verbose=0, shuffle=False, batch_size=1, callbacks=[earlyStop])\n",
    "        print('  --Making predictions and saving.')\n",
    "        test_predictions = lstm_model.predict(test_data) \n",
    "        \n",
    "        y_pred = scalers['label'].inverse_transform(test_predictions)\n",
    "        y_true = scalers['label'].inverse_transform(test_label)\n",
    "\n",
    "        test_metrics = get_metrics(y_true, y_pred)\n",
    "        \n",
    "        fdir = os.path.join(proj_path, catalog['results']['dir'], f'{str(test_end.date())}')\n",
    "        fname = os.path.join(fdir, f'lstm_exp2_{store_id}_{upc_id}.csv')\n",
    "        create_folder(fdir)\n",
    "\n",
    "        pd.DataFrame({'y_true': y_true,\n",
    "                      'y_pred': y_pred.flatten()}).to_csv(fname)\n",
    "        \n",
    "        with mlflow.start_run():\n",
    "            mlflow.log_param('model','lstm')\n",
    "            mlflow.log_param('experiment','exp2')\n",
    "            mlflow.log_param('fold_end',str(test_end.date()))\n",
    "            mlflow.log_params(best_params)\n",
    "            mlflow.log_params(params['lstm'])\n",
    "            mlflow.log_metrics(test_metrics)\n",
    "            mlflow.log_artifact(fname)\n",
    "            mlflow.log_params({'g_cat_state':True,\n",
    "                               'g_cat_us':True,\n",
    "                               'g_cc_state':True,\n",
    "                               'g_cc_us':True})\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
