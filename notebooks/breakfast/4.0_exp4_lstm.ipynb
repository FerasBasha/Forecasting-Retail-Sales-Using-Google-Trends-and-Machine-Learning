{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\feras\\Anaconda3\\envs\\ForecastingRetailSales\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\feras\\Anaconda3\\envs\\ForecastingRetailSales\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\feras\\Anaconda3\\envs\\ForecastingRetailSales\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\feras\\Anaconda3\\envs\\ForecastingRetailSales\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\feras\\Anaconda3\\envs\\ForecastingRetailSales\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\feras\\Anaconda3\\envs\\ForecastingRetailSales\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\feras\\Anaconda3\\envs\\ForecastingRetailSales\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\feras\\Anaconda3\\envs\\ForecastingRetailSales\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\feras\\Anaconda3\\envs\\ForecastingRetailSales\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\feras\\Anaconda3\\envs\\ForecastingRetailSales\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\feras\\Anaconda3\\envs\\ForecastingRetailSales\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\feras\\Anaconda3\\envs\\ForecastingRetailSales\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('CRITICAL')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import timedelta, datetime\n",
    "import itertools\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import yaml\n",
    "import mlflow\n",
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK, plotting, space_eval\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from scipy.stats.distributions import uniform, randint\n",
    "\n",
    "from glob import glob\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Get the current project path (where you open the notebook)\n",
    "# and go up two levels to get the project path\n",
    "current_dir = Path.cwd()\n",
    "proj_path = current_dir.parent.parent\n",
    "\n",
    "\n",
    "# make the code in src available to import in this notebook\n",
    "import sys\n",
    "sys.path.append(os.path.join(proj_path,'src'))\n",
    "\n",
    "from metrics import *\n",
    "from utils import *\n",
    "from scalers import *\n",
    "\n",
    "# Catalog contains all the paths related to datasets\n",
    "with open(os.path.join(proj_path, 'conf/catalog.yml'), \"r\") as f:\n",
    "    catalog = yaml.safe_load(f)['breakfast']\n",
    "    \n",
    "# Params contains all of the dataset creation parameters and model parameters\n",
    "with open(os.path.join(proj_path, 'conf/params.yml'), \"r\") as f:\n",
    "    params = yaml.safe_load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model(n_layers:int=1,init_units:int=50,n_unit_strategy:str='stable', \n",
    "              dropout_p:float=0.1, num_timesteps: int=8, num_series: int=5, \n",
    "              lr:float=0.001,optimizer:str='adam', loss:str='mape'):\n",
    "    '''\n",
    "    # Gives flexibility\n",
    "    # Explore depth vs width of the RNN model\n",
    "    https://stackoverflow.com/questions/59072728/what-is-the-rule-to-know-how-many-lstm-cells-and-how-many-units-in-each-lstm-cel\n",
    "    # there are no rule of thumb, but here the decrease strategy will devide by i the number of units at each layer.\n",
    "    \n",
    "    Args\n",
    "        num_timesteps: the number of lags in the dataframes\n",
    "        num_series: the number of time series, specify 1 if univariate\n",
    "        n_unit_strategy: two options, stable and decrease. decrease will decrease the  \n",
    "    '''\n",
    "    model = Sequential()\n",
    "    if n_unit_strategy == 'stable':\n",
    "        n_units = [init_units] * n_layers\n",
    "    if n_unit_strategy == 'decrease':\n",
    "        n_units = [max(int(init_units/i),1) for i in range(1,n_layers+1)]\n",
    "    \n",
    "    if n_layers > 1:\n",
    "        model.add(LSTM(n_units[0], return_sequences=True, input_shape=(num_timesteps, num_series)))\n",
    "        model.add(Dropout(dropout_p)) # Prevent overfitting\n",
    "        for i in range(1, n_layers):\n",
    "            model.add(LSTM(n_units[i], return_sequences=True))\n",
    "            model.add(Dropout(dropout_p)) \n",
    "        model.add(LSTM(n_units[-1]))\n",
    "        model.add(Dropout(dropout_p))\n",
    "    \n",
    "    else:\n",
    "        model.add(LSTM(n_units[0], return_sequences=False, input_shape=(num_timesteps, num_series)))\n",
    "        model.add(Dropout(dropout_p)) # Prevent overfitting\n",
    "\n",
    "    model.add(Dense(units=1))\n",
    "    \n",
    "    if optimizer == 'adam':\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    if optimizer == 'rmsprop':\n",
    "        opt = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "    \n",
    "    model.compile(optimizer=opt, loss=loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, lag_units, list_features:list=['UNITS'], g_features:list=[], lag_g_units:int=4):\n",
    "    \"\"\"\n",
    "    For every feature in the list of features, create lagged features\n",
    "    for a specified number of lagged units. The lenght must be the\n",
    "    same for all of the features. After creating the new features,\n",
    "    it filters on those and adds them to a list that will be \n",
    "    reshaped in the correct format.\n",
    "    \n",
    "    We go from a dataframe version:\n",
    "    \n",
    "    UNITS    | FEATURE_A | FEATURE_B |   ...\n",
    "    ---------|-----------|-----------|-------\n",
    "    \n",
    "    For every feature:\n",
    "        \n",
    "        Create the lagged features:\n",
    "\n",
    "        UNITS    | FEATURE_A | FEATURE_A-lag-1 |   ...\n",
    "        ---------|-----------|-----------------|-------  \n",
    "\n",
    "        Filter on each lagged feature and inverse columns:\n",
    "\n",
    "        FEATURE_A-lag-N | ... |  FEATURE_A-lag-1\n",
    "        ----------------|-----|------------------\n",
    "        \n",
    "        Convert to numpy as reshape by adding a dimension\n",
    "        (nrows, nlags) -> (nrows, nlags, 1)\n",
    "        \n",
    "    Concatenate each \"feature-specific matrix\" on the last dimension\n",
    "    \n",
    "    [(nrows, lagged-features-A, 1),\n",
    "     (nrows, lagged-features-B, 1),\n",
    "                  ...\n",
    "     (nrows, lagged-features-X, 1)]\n",
    "     \n",
    "     Results in: \n",
    "     \n",
    "     (nrows, nlags, N)\n",
    "    \n",
    "    \n",
    "    Note: We need to specify the column 'UNITS' in the list of features.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize a list\n",
    "    list_df = []\n",
    "    \n",
    "    for feature in (list_features+g_features):\n",
    "        # create new features that will have the convention lag-<col_name>-<lagged unit>\n",
    "        if feature in list_features:\n",
    "            make_lag_features(df, lag_units, col_name=feature, prefix_name=f'lag-{feature}',inplace=True)\n",
    "        if feature in g_features:\n",
    "            make_lag_features(df, lag_g_units, col_name=feature, prefix_name=f'lag-{feature}',inplace=True)\n",
    "        # filter columns that were just created\n",
    "        # we are only appending lag something, so not actual features\n",
    "        dummy = df.filter(like=f'lag-{feature}')\n",
    "        # invert columns left to right (lag-1 ... lag-N) -> (lag-N ... lag-1)\n",
    "        dummy = dummy[dummy.columns[::-1]]\n",
    "        # add an extra dimension\n",
    "        dummy = dummy.values.reshape(dummy.shape[0], dummy.shape[1], 1)\n",
    "        # add the matrice to a list where it will get concatenated at the end\n",
    "        list_df.append(dummy)\n",
    "        \n",
    "    \n",
    "\n",
    "    # matrices in list_df must have shape (n_rows, n_cols, 1)\n",
    "    # so we can concatenate on the last axis\n",
    "    x_features = np.concatenate(list_df, axis=2)\n",
    "    y_feature = np.array(df['UNITS'].values)\n",
    "\n",
    "    return x_features, y_feature, df['WEEK_END_DATE'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2277, 1600027527),\n",
       " (2277, 3800031838),\n",
       " (2277, 1111009477),\n",
       " (2277, 7192100339),\n",
       " (389, 1600027527),\n",
       " (389, 3800031838),\n",
       " (389, 1111009477),\n",
       " (389, 7192100339),\n",
       " (25229, 1600027527),\n",
       " (25229, 3800031838),\n",
       " (25229, 1111009477),\n",
       " (25229, 7192100339)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores = list(params['breakfast']['dataset']['store_ids'].keys())\n",
    "upcs = list(params['breakfast']['dataset']['upc_ids'].keys())\n",
    "store_upc_pairs = list(itertools.product(stores, upcs))\n",
    "store_upc_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing store 2277 upc 1600027527\n",
      "Processing range 2009-01-17 to 2011-01-29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:09<00:00,  6.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 46s, sys: 59.2 s, total: 4min 45s\n",
      "Wall time: 2min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Step 1: Read data\n",
    "merged_data = pd.read_csv(os.path.join(proj_path, catalog['output_dir']['dir'],\n",
    "                                       catalog['output_dir']['merged']))\n",
    "merged_data['WEEK_END_DATE'] = pd.to_datetime(merged_data['WEEK_END_DATE'])\n",
    "merged_data['WEEK_END_DATE'] = merged_data['WEEK_END_DATE'] + timedelta(days=3)\n",
    "\n",
    "# Step 2: Create list of stores and upc pairs\n",
    "stores = list(params['breakfast']['dataset']['store_ids'].keys())\n",
    "upcs = list(params['breakfast']['dataset']['upc_ids'].keys())\n",
    "store_upc_pairs = list(itertools.product(stores, upcs))\n",
    "\n",
    "# Create date folds\n",
    "date_ranges = make_dates(params['breakfast']['experiment_dates'])\n",
    "\n",
    "\n",
    "for store_id, upc_id in store_upc_pairs: \n",
    "    print(f'Processing store {store_id} upc {upc_id}')\n",
    "    \n",
    "    create_folder(os.path.join(proj_path, 'mlruns'))\n",
    "    mlflow.set_tracking_uri('../../mlruns')\n",
    "    mlflow.set_experiment(f'{store_id}_{upc_id}')\n",
    "    \n",
    "    # Iterate over each period, unpack tuple in each variable.\n",
    "    # in each of the period, we will find the best set of parameters,\n",
    "    # which will represent the time-series cross validation methodology\n",
    "    for _, train_start, train_end, valid_start, valid_end, test_start, test_end in date_ranges.itertuples():\n",
    "        print(f'Processing range {str(train_start.date())} to {str(test_end.date())}')\n",
    "        \n",
    "        \n",
    "        list_features = ['UNITS','VISITS','HHS','FEATURE','DISPLAY']\n",
    "        # Filter data here\n",
    "        # Prepare the dataset for one UPC and one store, currently no-scaling\n",
    "        filtered_data = merged_data[(merged_data['STORE_NUM']==store_id) &\n",
    "                                     (merged_data['UPC']==upc_id)][list_features+['WEEK_END_DATE']].copy()\n",
    "\n",
    "        ws = params['lstm']['window_size']\n",
    "        # ┌────────────────── GOOGLE TRENDS STARTS HERE ────────────────────┐\n",
    "        # │\n",
    "        # Get product category and state\n",
    "        state = params['breakfast']['dataset']['store_ids'][store_id]\n",
    "        cat = params['breakfast']['dataset']['upc_ids'][upc_id]\n",
    "\n",
    "        # Do we need an epty glist ?\n",
    "        glist = []\n",
    "        # List comprehension to iteratively read in the correct directory the google\n",
    "        # trends csv files that match the appropriate pattern of * (everything) in respective state\n",
    "        # returns a list of pandas dataframes where after we concat on the column axis using the index_col\n",
    "        # After that, we reset the index, to get the Week as a column and convert to datetime\n",
    "        # Finaly, we add 6 days as the google trends hits represents the end of week and corresponds\n",
    "        # to Breakfast At the Frat dates\n",
    "        glist = [\n",
    "            pd.read_csv(path, \n",
    "                        skiprows= 3, \n",
    "                        index_col= 'Week', \n",
    "                        names= ['Week', parse_name([str(proj_path), f'/data/03_external/{cat}/', '.csv'],path)]) \n",
    "            for path in glob(str(proj_path / f'data/03_external/{cat}/*_{state}.csv'))\n",
    "        ]\n",
    "        glist_us =  [\n",
    "            pd.read_csv(path, \n",
    "                        skiprows= 3, \n",
    "                        index_col= 'Week', \n",
    "                        names= ['Week', parse_name([str(proj_path), f'/data/03_external/{cat}/', '.csv'],path)]) \n",
    "            for path in glob(str(proj_path / f'data/03_external/{cat}/*_US.csv'))\n",
    "        ]\n",
    "        # Cross Category\n",
    "        glist_cc_state =  [\n",
    "            pd.read_csv(path, \n",
    "                        skiprows= 3, \n",
    "                        index_col= 'Week', \n",
    "                        names= ['Week', parse_name([str(proj_path), f'/data/03_external/cross_category/', '.csv'],path)]) \n",
    "            for path in glob(str(proj_path / f'data/03_external/cross_category/*_{state}.csv'))\n",
    "        ]\n",
    "        glist_cc_us =  [\n",
    "            pd.read_csv(path, \n",
    "                        skiprows= 3, \n",
    "                        index_col= 'Week', \n",
    "                        names= ['Week', parse_name([str(proj_path), f'/data/03_external/cross_category/', '.csv'],path)]) \n",
    "            for path in glob(str(proj_path / f'data/03_external/cross_category/*_US.csv'))\n",
    "        ]\n",
    "        # This is where you decide on whether to add those series, comment if you do not\n",
    "        # The series should all have unique names, and is based on name of the file\n",
    "        glist.extend(glist_us)\n",
    "        glist.extend(glist_cc_state)\n",
    "        glist.extend(glist_cc_us)\n",
    "\n",
    "        # Combine all of the series into one, column-wise\n",
    "        gdata = pd.concat(glist, axis=1).reset_index()\n",
    "        gdata = fix_col_syntax(gdata)\n",
    "        # reset index will always set the Week as the first column, so select after that date.\n",
    "        gdata_feature_cols = gdata.columns[1:].to_list()\n",
    "        gdata['Week'] = pd.to_datetime(gdata['Week'])\n",
    "        gdata['Week'] = gdata['Week'] + timedelta(days=6)\n",
    "        # Join on filtered_data\n",
    "        filtered_data = filtered_data.merge(gdata, left_on='WEEK_END_DATE', right_on='Week', how='left').drop(columns='Week')\n",
    "        # │ \n",
    "        # └────────────────────── GOOGLE TRENDS ENDS HERE ─────────────────────────┘\n",
    "        \n",
    "        # Prepare data, pass in all of the data, it will filter it\n",
    "        data, label, dates = prepare_data(filtered_data, \n",
    "                                          ws, \n",
    "                                          list_features=list_features,\n",
    "                                          g_features=gdata_feature_cols,\n",
    "                                          lag_g_units= params['lstm']['gtrends_window_size'])\n",
    "        \n",
    "        # Because we are working with numpy now, it's easier to select the indexes\n",
    "        # Will need date indexes \n",
    "        date_series = pd.Series(dates)        \n",
    "        \n",
    "        train_series_idx = date_series[(date_series>=train_start) &\n",
    "                                       (date_series<=train_end)].index\n",
    "        valid_series_idx = date_series[(date_series>=valid_start) &\n",
    "                                       (date_series<=valid_end)].index\n",
    "        test_series_idx = date_series[(date_series>=test_start) &\n",
    "                                      (date_series<=test_end)].index\n",
    "        train_data = data[train_series_idx].copy()\n",
    "        train_label = label[train_series_idx].copy()\n",
    "        valid_data = data[valid_series_idx].copy()\n",
    "        valid_label = label[valid_series_idx].copy()\n",
    "        test_data = data[test_series_idx].copy()\n",
    "        test_label = label[test_series_idx].copy()   \n",
    "        \n",
    "        # The scaling of 0 to 1 creates issue with the loss function, as the minimum will be 0 and maximum 1. \n",
    "        # a bad solution was to drop the row that has 0, but doesn't seem to help. \n",
    "        # another solution is to log the values\n",
    "        \n",
    "        scalers = {}\n",
    "        # The last two features are already bounded 0 to 1, we subtract 2 from the shape which represent 'FEATURE', 'DISPLAY'\n",
    "        # for feature_col in range(train_data.shape[-1]-2):\n",
    "        for feature_col in range(train_data.shape[-1]):\n",
    "            scalers[feature_col] = NormalizeScalerDf()\n",
    "            train_data[:,:,feature_col] = scalers[feature_col].fit_transform(train_data[:,:,feature_col])\n",
    "            valid_data[:,:,feature_col] = scalers[feature_col].transform(valid_data[:,:,feature_col])\n",
    "            test_data[:,:,feature_col] = scalers[feature_col].transform(test_data[:,:,feature_col])\n",
    "        scalers['label'] = NormalizeScalerDf()\n",
    "        train_label = scalers['label'].fit_transform(train_label)\n",
    "        valid_label = scalers['label'].transform(valid_label)\n",
    "        test_label = scalers['label'].transform(test_label)\n",
    "\n",
    "        # Replace NaN's with 0's which are caused by the lagged values. This was done after the scaling to avoid issues\n",
    "        train_data = np.nan_to_num(train_data)\n",
    "        valid_data = np.nan_to_num(valid_data)\n",
    "        test_data = np.nan_to_num(test_data)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Initialize some of the parameters\n",
    "        ws = params['lstm']['window_size']\n",
    "        dp = params['lstm']['dropout']\n",
    "        units_strategy = params['lstm']['units_strategy']\n",
    "        optimizers = params['lstm']['optimizers']\n",
    "        losses = params['lstm']['loss']\n",
    "        num_series = train_data.shape[2]   \n",
    "\n",
    "        # Step 3: Define a random search for these parameters, for hyperparameter tuning\n",
    "#         random_number_generator = np.random.RandomState(0) \n",
    "        param_grid = {'lr': uniform(loc=0.0001, scale=0.01),\n",
    "                      'init_units': randint(low=5, high= 100),\n",
    "                      'total_layers': randint(low=1, high=2)}\n",
    "        param_list = list(ParameterSampler(param_grid, n_iter=params['lstm']['search_iter'], random_state=3))\n",
    "        \n",
    "        \n",
    "        # Perform hyperparameter search\n",
    "        res = []\n",
    "        for param_dict in tqdm(param_list):\n",
    "            lr = param_dict['lr']\n",
    "            init_units = param_dict['init_units']\n",
    "            tot_lay = param_dict['total_layers']\n",
    "        \n",
    "            # Explain here, stop when validation loss doesn't decrease for two steps/iterations\n",
    "            earlyStop=EarlyStopping(monitor=\"val_loss\", verbose=0, mode='min', patience=2)\n",
    "            # Initialize the model and train\n",
    "            lstm_model = set_model(n_layers=tot_lay, init_units=init_units,\n",
    "                                   n_unit_strategy=units_strategy, dropout_p=dp,\n",
    "                                   num_timesteps=ws, num_series=num_series,\n",
    "                                   lr=lr, optimizer=optimizers, loss=losses)\n",
    "            history = lstm_model.fit(train_data, train_label, validation_data=(valid_data, valid_label), \n",
    "                                     epochs=100, verbose=0, shuffle=False, batch_size=1, callbacks=[earlyStop])\n",
    "\n",
    "            # _ = lstm_model.predict(np.expand_dims(valid_data[0],axis=0))[0]\n",
    "            # There are many warnings when making predictions, thus I use a comprehension loop\n",
    "            # val_predictions = np.array([lstm_model.predict(np.expand_dims(dd,axis=0))[0] for dd in valid_data])\n",
    "            val_predictions = lstm_model.predict(valid_data)\n",
    "            \n",
    "            y_pred = scalers['label'].inverse_transform(val_predictions)\n",
    "            y_true = scalers['label'].inverse_transform(valid_label)\n",
    "\n",
    "            test_metrics = get_metrics(y_true, y_pred)\n",
    "            \n",
    "            val_mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "\n",
    "            param_res = {'learning_rate':lr,\n",
    "                         'init_units':init_units,\n",
    "                         'n_layers':tot_lay,\n",
    "                         'val_mape':val_mape}\n",
    "\n",
    "            res.append(param_res)\n",
    "        \n",
    "        # Select the optimal parameters\n",
    "        best_params = pd.DataFrame(res).sort_values(by='val_mape', ascending=True).iloc[0]\n",
    "\n",
    "        lstm_model = set_model(n_layers=int(best_params['n_layers']),\n",
    "                               init_units=best_params['init_units'],\n",
    "                               n_unit_strategy=units_strategy,\n",
    "                               dropout_p=dp,\n",
    "                               num_timesteps=ws,\n",
    "                               num_series=num_series,\n",
    "                               lr=best_params['learning_rate'],\n",
    "                               optimizer=optimizers,\n",
    "                               loss=losses)\n",
    "\n",
    "        earlyStop=EarlyStopping(monitor=\"val_loss\", verbose=0, mode='min', patience=2)\n",
    "        history = lstm_model.fit(train_data, train_label, validation_data=(valid_data, valid_label),\n",
    "                                 epochs=100, verbose=0, shuffle=False, batch_size=1, callbacks=[earlyStop])\n",
    "\n",
    "        test_predictions = lstm_model.predict(test_data) \n",
    "        \n",
    "        y_pred = scalers['label'].inverse_transform(test_predictions)\n",
    "        y_true = scalers['label'].inverse_transform(test_label)\n",
    "\n",
    "        \n",
    "        fdir = os.path.join(proj_path, catalog['results']['dir'], f'{str(test_end.date())}')\n",
    "        fname = os.path.join(fdir, f'lstm_exp4_{store_id}_{upc_id}.csv')\n",
    "        create_folder(fdir)\n",
    "\n",
    "        pd.DataFrame({'y_true': y_true,\n",
    "                      'y_pred': y_pred.flatten()}).to_csv(fname)\n",
    "        \n",
    "        with mlflow.start_run():\n",
    "            mlflow.log_param('model','lstm')\n",
    "            mlflow.log_param('experiment','exp4')\n",
    "            mlflow.log_params(best_params)\n",
    "            mlflow.log_param('fold_end',str(test_end.date()))\n",
    "            mlflow.log_params(params['lstm'])\n",
    "            mlflow.log_metrics(test_metrics)\n",
    "            mlflow.log_artifact(fname)\n",
    "            mlflow.log_params({'g_cat_state':True,\n",
    "                               'g_cat_us':True,\n",
    "                               'g_cc_state':True,\n",
    "                               'g_cc_us':True})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
