{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline II: Facebook Prophet\n",
    "\n",
    "[[Prophet]](https://facebook.github.io/prophet/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta, datetime\n",
    "import itertools\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import mlflow\n",
    "from fbprophet import Prophet\n",
    "import yaml\n",
    "\n",
    "# Get the current project path (where you open the notebook)\n",
    "# and go up two levels to get the project path\n",
    "current_dir = Path.cwd()\n",
    "proj_path = current_dir.parent.parent\n",
    "\n",
    "# make the code in src available to import in this notebook\n",
    "import sys\n",
    "sys.path.append(os.path.join(proj_path,'src'))\n",
    "\n",
    "from metrics import mean_absolute_percentage_error, get_metrics\n",
    "from utils import make_dates, create_folder\n",
    "\n",
    "# Catalog contains all the paths related to datasets\n",
    "with open(os.path.join(proj_path, 'conf/catalog.yml'), \"r\") as f:\n",
    "    catalog = yaml.safe_load(f)['breakfast']\n",
    "    \n",
    "# Params contains all of the dataset creation parameters and model parameters\n",
    "with open(os.path.join(proj_path, 'conf/params.yml'), \"r\") as f:\n",
    "    params = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing store 2277 upc 1600027527\n",
      "Processing range 2009-01-17 to 2011-01-29\n"
     ]
    }
   ],
   "source": [
    "# mlflow.set_tracking_uri(os.path.join(proj_path, 'logging'))\n",
    "# mlflow.set_experiment('experiment-prophet-delete')\n",
    "# Create mlflow tracking folder\n",
    "create_folder(os.path.join(proj_path, 'mlruns'))\n",
    "\n",
    "# Step 1: Read data\n",
    "merged_data = pd.read_csv(os.path.join(proj_path, \n",
    "                                       catalog['output_dir']['dir'], \n",
    "                                       catalog['output_dir']['merged']))\n",
    "merged_data['WEEK_END_DATE'] = pd.to_datetime(merged_data['WEEK_END_DATE'])\n",
    "merged_data['WEEK_END_DATE'] = merged_data['WEEK_END_DATE'] + timedelta(days=3)\n",
    "\n",
    "# Step2: Create date folds\n",
    "date_ranges = make_dates(params['breakfast']['experiment_dates'])\n",
    "\n",
    "# stores = params['breakfast']['dataset']['store_ids']\n",
    "# upcs = params['breakfast']['dataset']['upcs']\n",
    "# full_search = list(itertools.product(stores, upcs))\n",
    "\n",
    "\n",
    "# Step 3: Iterate over each store and upc pair.\n",
    "# For each pair, iterate over each period (fold), find the optimal\n",
    "# set of parameters for that fold and make the predictions\n",
    "# on the test period\n",
    "stores = list(params['breakfast']['dataset']['store_ids'].keys())\n",
    "upcs = list(params['breakfast']['dataset']['upc_ids'].keys())\n",
    "store_upc_pairs = list(itertools.product(stores, upcs))\n",
    "\n",
    "for store_id, upc_id in store_upc_pairs: \n",
    "    print(f'Processing store {store_id} upc {upc_id}')\n",
    "    mlflow.set_tracking_uri(os.path.join(proj_path, 'mlruns'))\n",
    "    mlflow.set_experiment(f'{store_id}_{upc_id}')\n",
    "    \n",
    "    mlflow.runName = 'prophet_' + str(datetime.today())[:19]\n",
    "    # Iterate over each period, unpack tuple in each variable.\n",
    "    # in each of the period, we will find the best set of parameters,\n",
    "    # which will represent the time-series cross validation methodology\n",
    "    for _, train_start, train_end, valid_start, valid_end, test_start, test_end in date_ranges.itertuples():\n",
    "        print(f'Processing range {str(train_start.date())} to {str(test_end.date())}')\n",
    "\n",
    "        train_x = merged_data[(merged_data['WEEK_END_DATE']>=train_start) &\n",
    "                                  (merged_data['WEEK_END_DATE']<=valid_end) &\n",
    "                                  (merged_data['STORE_NUM']==store_id) &\n",
    "                                  (merged_data['UPC']==upc_id)][['WEEK_END_DATE','UNITS']]\n",
    "        # Doesn't need a validation period.\n",
    "        test_y = merged_data[(merged_data['WEEK_END_DATE']>=test_start) &\n",
    "                             (merged_data['WEEK_END_DATE']<=test_end) &\n",
    "                             (merged_data['STORE_NUM']==store_id) &\n",
    "                             (merged_data['UPC']==upc_id)][['WEEK_END_DATE','UNITS']]\n",
    "        # Prophet expects two columns, one with the label 'ds' for the dates and y for the values\n",
    "        train_x = train_x.rename(columns={'WEEK_END_DATE':'ds', 'UNITS':'y'})\n",
    "        test_y = test_y.rename(columns={'WEEK_END_DATE':'ds', 'UNITS':'y'})\n",
    "\n",
    "        # Iterate over the periods to make next-day forecasts\n",
    "        predictions = []\n",
    "        for i in range(test_y.shape[0]):\n",
    "\n",
    "            #Instantiate a new Prophet object that represents the model\n",
    "            model = Prophet(weekly_seasonality=True,\n",
    "                            yearly_seasonality=True,\n",
    "                            daily_seasonality=False)\n",
    "\n",
    "            #Call the built-in holiday collection for US to be included in the model\n",
    "            model.add_country_holidays(country_name='US')\n",
    "\n",
    "            # Fit the FB Prohpet Model\n",
    "            model.fit(pd.concat([train_x.iloc[i:], test_y.iloc[:i]]))\n",
    "            future = model.make_future_dataframe(periods=1, freq='7D')\n",
    "            fcst = model.predict(future)['yhat'].iloc[-1]\n",
    "            predictions.append(fcst)\n",
    "        \n",
    "        run_metrics = get_metrics(test_y['y'].values, predictions)\n",
    "        \n",
    "        # store predictions\n",
    "        fdir = os.path.join(proj_path, catalog['results']['dir'], f'{str(test_end.date())}')\n",
    "        fname = os.path.join(fdir, f'prophet_{store_id}_{upc_id}.csv')\n",
    "        create_folder(fdir)\n",
    "\n",
    "        test_y['preds'] = predictions\n",
    "        \n",
    "        test_y.to_csv(fname)\n",
    "        with mlflow.start_run():\n",
    "            mlflow.log_artifact(fname)\n",
    "            mlflow.log_param('model','prophet')\n",
    "            mlflow.log_metrics(run_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
