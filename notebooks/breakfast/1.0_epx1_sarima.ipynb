{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline I: SARIMA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import itertools\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import mlflow\n",
    "from datetime import datetime\n",
    "\n",
    "# Get the current project path (where you open the notebook)\n",
    "# and go up two levels to get the project path\n",
    "current_dir = Path.cwd()\n",
    "proj_path = current_dir.parent.parent\n",
    "\n",
    "# make the code in src available to import in this notebook\n",
    "import sys\n",
    "sys.path.append(os.path.join(proj_path, 'src'))\n",
    "\n",
    "# Custom functions and classes\n",
    "from sarima import SklearnSarima\n",
    "from utils import make_dates, create_folder\n",
    "from metrics import get_metrics\n",
    "\n",
    "# Catalog contains all the paths related to datasets\n",
    "with open(os.path.join(proj_path, 'conf/catalog.yml'), \"r\") as f:\n",
    "    catalog = yaml.safe_load(f)['breakfast']\n",
    "    \n",
    "# Params contains all of the dataset creation parameters and model parameters\n",
    "with open(os.path.join(proj_path, 'conf/params.yml'), \"r\") as f:\n",
    "    params = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the data, convert to a proper datetime format and apply correction\n",
    "merged_data = pd.read_csv(os.path.join(proj_path,\n",
    "                                       catalog['output_dir']['dir'], \n",
    "                                       catalog['output_dir']['merged']))\n",
    "merged_data['WEEK_END_DATE'] = pd.to_datetime(merged_data['WEEK_END_DATE'])\n",
    "merged_data['WEEK_END_DATE'] = merged_data['WEEK_END_DATE'] + timedelta(days=3)\n",
    "\n",
    "# Step 2: Create date folds\n",
    "date_ranges = make_dates(params['breakfast']['experiment_dates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are predicting on Saturday\n",
    "\n",
    "# Fold 1:\n",
    "\n",
    "# validat Saturday 2010-12-11\t  -> Last Google Trends hits covers Sunday 2010-11-28 to Saturday 2010-12-04\n",
    "#                                    Thus we add + 6 days, so that the date for that period will be labelled\n",
    "#                                    2010-12-11 and will be used to predict 2010-12-11.\n",
    "#                                 -> For Breakfast at the Frat, we need to add + 3 days so the days match\n",
    "#                                    and corresponds to end of week. The sales from previous week can only be\n",
    "#                                    used. \n",
    "\n",
    "# validat Saturday 2010-12-18\t\n",
    "# validat Saturday 2010-12-25\n",
    "# validat Saturday 2011-01-01\t\n",
    "\n",
    "# predict Saturday 2011-01-08\n",
    "# predict Saturday 2011-01-15\n",
    "# predict Saturday 2011-01-22\n",
    "# predict Saturday 2011-01-29\n",
    "\n",
    "# Fold 2:\n",
    "\n",
    "# validat Saturday 2011-01-08\n",
    "# validat Saturday 2011-01-15\n",
    "# validat Saturday 2011-01-22\n",
    "# validat Saturday 2011-01-29\n",
    "\n",
    "# predict Saturday 2011-02-05\n",
    "# predict Saturday 2011-02-12\n",
    "# predict Saturday 2011-02-19\n",
    "# predict Saturday 2011-02-26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_start</th>\n",
       "      <th>train_end</th>\n",
       "      <th>valid_start</th>\n",
       "      <th>valid_end</th>\n",
       "      <th>test_start</th>\n",
       "      <th>test_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-17</td>\n",
       "      <td>2010-12-04</td>\n",
       "      <td>2010-12-11</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>2011-01-08</td>\n",
       "      <td>2011-01-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-02-14</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>2011-01-08</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>2011-02-05</td>\n",
       "      <td>2011-02-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-03-14</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>2011-02-05</td>\n",
       "      <td>2011-02-26</td>\n",
       "      <td>2011-03-05</td>\n",
       "      <td>2011-03-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-04-11</td>\n",
       "      <td>2011-02-26</td>\n",
       "      <td>2011-03-05</td>\n",
       "      <td>2011-03-26</td>\n",
       "      <td>2011-04-02</td>\n",
       "      <td>2011-04-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-05-09</td>\n",
       "      <td>2011-03-26</td>\n",
       "      <td>2011-04-02</td>\n",
       "      <td>2011-04-23</td>\n",
       "      <td>2011-04-30</td>\n",
       "      <td>2011-05-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2009-06-06</td>\n",
       "      <td>2011-04-23</td>\n",
       "      <td>2011-04-30</td>\n",
       "      <td>2011-05-21</td>\n",
       "      <td>2011-05-28</td>\n",
       "      <td>2011-06-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2009-07-04</td>\n",
       "      <td>2011-05-21</td>\n",
       "      <td>2011-05-28</td>\n",
       "      <td>2011-06-18</td>\n",
       "      <td>2011-06-25</td>\n",
       "      <td>2011-07-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2009-08-01</td>\n",
       "      <td>2011-06-18</td>\n",
       "      <td>2011-06-25</td>\n",
       "      <td>2011-07-16</td>\n",
       "      <td>2011-07-23</td>\n",
       "      <td>2011-08-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2009-08-29</td>\n",
       "      <td>2011-07-16</td>\n",
       "      <td>2011-07-23</td>\n",
       "      <td>2011-08-13</td>\n",
       "      <td>2011-08-20</td>\n",
       "      <td>2011-09-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2009-09-26</td>\n",
       "      <td>2011-08-13</td>\n",
       "      <td>2011-08-20</td>\n",
       "      <td>2011-09-10</td>\n",
       "      <td>2011-09-17</td>\n",
       "      <td>2011-10-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2009-10-24</td>\n",
       "      <td>2011-09-10</td>\n",
       "      <td>2011-09-17</td>\n",
       "      <td>2011-10-08</td>\n",
       "      <td>2011-10-15</td>\n",
       "      <td>2011-11-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2009-11-21</td>\n",
       "      <td>2011-10-08</td>\n",
       "      <td>2011-10-15</td>\n",
       "      <td>2011-11-05</td>\n",
       "      <td>2011-11-12</td>\n",
       "      <td>2011-12-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2009-12-19</td>\n",
       "      <td>2011-11-05</td>\n",
       "      <td>2011-11-12</td>\n",
       "      <td>2011-12-03</td>\n",
       "      <td>2011-12-10</td>\n",
       "      <td>2011-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_start  train_end valid_start  valid_end test_start   test_end\n",
       "0   2009-01-17 2010-12-04  2010-12-11 2011-01-01 2011-01-08 2011-01-29\n",
       "1   2009-02-14 2011-01-01  2011-01-08 2011-01-29 2011-02-05 2011-02-26\n",
       "2   2009-03-14 2011-01-29  2011-02-05 2011-02-26 2011-03-05 2011-03-26\n",
       "3   2009-04-11 2011-02-26  2011-03-05 2011-03-26 2011-04-02 2011-04-23\n",
       "4   2009-05-09 2011-03-26  2011-04-02 2011-04-23 2011-04-30 2011-05-21\n",
       "5   2009-06-06 2011-04-23  2011-04-30 2011-05-21 2011-05-28 2011-06-18\n",
       "6   2009-07-04 2011-05-21  2011-05-28 2011-06-18 2011-06-25 2011-07-16\n",
       "7   2009-08-01 2011-06-18  2011-06-25 2011-07-16 2011-07-23 2011-08-13\n",
       "8   2009-08-29 2011-07-16  2011-07-23 2011-08-13 2011-08-20 2011-09-10\n",
       "9   2009-09-26 2011-08-13  2011-08-20 2011-09-10 2011-09-17 2011-10-08\n",
       "10  2009-10-24 2011-09-10  2011-09-17 2011-10-08 2011-10-15 2011-11-05\n",
       "11  2009-11-21 2011-10-08  2011-10-15 2011-11-05 2011-11-12 2011-12-03\n",
       "12  2009-12-19 2011-11-05  2011-11-12 2011-12-03 2011-12-10 2011-12-31"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dates(params['breakfast']['experiment_dates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'store_ids': {389: 'KY', 2277: 'OH', 25229: 'TX'},\n",
      " 'upc_ids': {1111009477: 'bag_snacks',\n",
      "             1600027527: 'cold_cereal',\n",
      "             3800031838: 'cold_cereal',\n",
      "             7192100339: 'frozen_pizza'}}\n"
     ]
    }
   ],
   "source": [
    "pprint(params['breakfast']['dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2277, 389, 25229]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(params['breakfast']['dataset']['store_ids'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding best parameters:   2%|▏         | 4/192 [00:00<00:05, 32.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing store 2277 upc 1600027527\n",
      "INFO: '2277_1600027527' does not exist. Creating a new experiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding best parameters: 100%|██████████| 192/192 [00:15<00:00, 12.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# Setup MLFLOW\n",
    "# One experiment will be store_id + upc_id\n",
    "# Initialize experiment logging location\n",
    "# Create mlflow tracking folder\n",
    "create_folder(os.path.join(proj_path, 'mlruns'))\n",
    "\n",
    "# Step 1: Load the data, convert to a proper datetime format and apply correction\n",
    "merged_data = pd.read_csv(os.path.join(proj_path,\n",
    "                                       catalog['output_dir']['dir'],  \n",
    "                                       catalog['output_dir']['merged']))\n",
    "merged_data['WEEK_END_DATE'] = pd.to_datetime(merged_data['WEEK_END_DATE'])\n",
    "merged_data['WEEK_END_DATE'] = merged_data['WEEK_END_DATE'] + timedelta(days=3)\n",
    "\n",
    "# Step 2: Create date folds\n",
    "date_ranges = make_dates(params['breakfast']['experiment_dates'])\n",
    "\n",
    "# Step 3: Iterate over each store and upc pair.\n",
    "# For each pair, iterate over each period (fold), find the optimal\n",
    "# set of parameters for that fold and make the predictions\n",
    "# on the test period\n",
    "stores = list(params['breakfast']['dataset']['store_ids'].keys())\n",
    "upcs = list(params['breakfast']['dataset']['upc_ids'].keys())\n",
    "store_upc_pairs = list(itertools.product(stores, upcs))\n",
    "\n",
    "for store_id, upc_id in store_upc_pairs: \n",
    "    print(f'Processing store {store_id} upc {upc_id}')\n",
    "    mlflow.set_tracking_uri(os.path.join(proj_path, 'mlruns'))\n",
    "    mlflow.set_experiment(f'{store_id}_{upc_id}')\n",
    "    \n",
    "    # Iterate over each period, unpack tuple in each variable.\n",
    "    # in each of the period, we will find the best set of parameters,\n",
    "    # which will represent the time-series cross validation methodology\n",
    "    for _, train_start, train_end, valid_start, valid_end, test_start, test_end in date_ranges.itertuples():\n",
    "        \n",
    "        # Define set of parameters for SARIMA\n",
    "        p = d = q = range(0, 2)\n",
    "        pdq = list(itertools.product(p, d, q))\n",
    "        spdq = list(itertools.product(p, d, q, [2,3,4]))\n",
    "        all_params = list(itertools.product(pdq, spdq))\n",
    "    \n",
    "        # Step 4: Filter data for the specific store and upc pair and dates.\n",
    "        # The dates are inclusive\n",
    "        train_x = merged_data[(merged_data['WEEK_END_DATE']>=train_start) &\n",
    "                              (merged_data['WEEK_END_DATE']<=train_end) &\n",
    "                              (merged_data['STORE_NUM']==store_id) &\n",
    "                              (merged_data['UPC']==upc_id)]['UNITS']\n",
    "        valid_y = merged_data[(merged_data['WEEK_END_DATE']>=valid_start) &\n",
    "                              (merged_data['WEEK_END_DATE']<=valid_end) &\n",
    "                              (merged_data['STORE_NUM']==store_id) &\n",
    "                              (merged_data['UPC']==upc_id)]['UNITS']\n",
    "        test_y = merged_data[(merged_data['WEEK_END_DATE']>=test_start) &\n",
    "                             (merged_data['WEEK_END_DATE']<=test_end) &\n",
    "                             (merged_data['STORE_NUM']==store_id) &\n",
    "                             (merged_data['UPC']==upc_id)]['UNITS']\n",
    "    \n",
    "        # Step 5: Initiate the model with the training data to use to fit\n",
    "        # Then use the fit_best_params that will iterate over all parameter combinations\n",
    "        # and evaluate which performs the best on the valid data using MAPE to\n",
    "        # evaluate the predictions. Then it will select the parameters that minimized\n",
    "        # this metric and fit using the training and validation data.\n",
    "        model = SklearnSarima(train_x.values)\n",
    "        model.fit_best_params(valid_y.values, all_params)\n",
    "    \n",
    "        # Step 6: Make the predictions. There are two options, a: forecast for four days\n",
    "        # and b: forecast for one day, and always use the most relevant data to re-fit\n",
    "        # the model using the same parameters found using the fit_best_params method.\n",
    "        # The prefix lt is used to indicate option a and nd indicates option b\n",
    "        # For option a, we only need to provide how many steps to forecast, while option\n",
    "        # b needs the actual test values, to be able to re-fit every day\n",
    "        lt_predictions = model.predict(test_y.values.size)\n",
    "        nd_predictions = model.fit_predict(test_y.values)\n",
    "    \n",
    "        # Step 7: Calculate the metrics for both forecasting techniques.\n",
    "        # metrics used are: MSE, MAPE, R2, WAPE and RMSE\n",
    "        lt_metrics = get_metrics(test_y.values, lt_predictions)\n",
    "        nd_metrics = get_metrics(test_y.values, nd_predictions)\n",
    "        used_params = model.get_params()\n",
    "    \n",
    "        # Step 8: Store the actual predictions in the respective folder.\n",
    "        # Also ensure that the folder exists by calling create_folder. If it does it will\n",
    "        # not do anything.\n",
    "        fdir = os.path.join(proj_path, catalog['results']['dir'], f'{str(test_end.date())}')\n",
    "        fname = os.path.join(fdir, f'sarima_{store_id}_{upc_id}.csv')\n",
    "        create_folder(fdir)\n",
    "\n",
    "        save_data = pd.DataFrame({'y_true':test_y.values,\n",
    "                                  'y_pred_lt':lt_predictions,\n",
    "                                  'y_pred_nd':np.array(nd_predictions).flatten(),\n",
    "                                  'dates':merged_data[(merged_data['WEEK_END_DATE']>=test_start) &\n",
    "                                                     (merged_data['WEEK_END_DATE']<=test_end) &\n",
    "                                                     (merged_data['STORE_NUM']==store_id) &\n",
    "                                                     (merged_data['UPC']==upc_id)]['WEEK_END_DATE'].values})\n",
    "        save_data.to_csv(fname)\n",
    "        with mlflow.start_run():\n",
    "            mlflow.log_artifact(fname)\n",
    "            mlflow.log_params(used_params)\n",
    "            mlflow.log_metrics(nd_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
